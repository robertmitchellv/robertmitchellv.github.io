[
  {
    "objectID": "projects/2020-04-covid19-dashboard/index.html",
    "href": "projects/2020-04-covid19-dashboard/index.html",
    "title": "LA County COVID-19 Dashboard",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "vitae.html",
    "href": "vitae.html",
    "title": "robertmitchellv",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me.\n\n\n\n\n\n\n I am dedicated and hard working–a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n\n\n\n\n\n\n\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering–specifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‘tidying’ data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I’m honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS\n\n\n\n\n\n\n University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006–2007)\n\n\n\n\n\n\n\n\n\n\n using expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\n provides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr. Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\n work closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\n building a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\n led DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\n continuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\n spearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\n help prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\n work with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\n liaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population–up to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\n assist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\n responsible for calculating program milestones and incentives for state billing–working with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\n act as bridge between IT, data team, evaluation team, and leadership on all data related matters–working to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\n provide strategic oversight to organization’s data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\n responsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\n moved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\n liaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\n prepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\n oversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\n leveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\n converted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\n liaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\n provide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\n leverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\n explored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\n actively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy–championed the role data could play in program development, intervention, and outcomes\n\n\n prepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\n assist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\n gathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\n performed statistical analysis of health survey data using Python–we had 200 respondents and were able to make programmatic changes based on our findings\n\n\n developed and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\n implemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\n planed and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\n provided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\n prepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\n project troubleshooter par excellence.\n\n\n\n\n\n\n\n\n responsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\n worked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\n researched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\n point-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\n researched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\n participated in regular trainings, program evaluation, and program development\n\n\n designed and edited Peer Advocate zine for commercial printing\n\n\n improved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\n designed and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\n created new social media presence for (then) new platform Vine to promote library collections and services\n\n\n designed online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\n facilitated presentation on approach and ideas to UCLA Library’s social media steering committee\n\n\n operated multi-tiered social media platform apparatus\n\n\n\n\n\n\n\n\n\n\n\n Researched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\n Designed, proofed, prepared, and labored over Peer Advocate ’Zine for Skid Row Housing Trust’s Hilton Project ’Zine “I Got You”, which will be released during a end of grant event for all project participants\n\n\n Was asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries–the final project was the “Top 10 Things” flier, which was a successful campaign to engage students in library programming and services\n\n\n Was recognized by two articles online for work promoting library services and collections through early-adoption of Vine\n\n\n\n\n\n Learning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\n Coding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can–I am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\n Design: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\n Exploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: 2022-07-22"
  },
  {
    "objectID": "talks/2018-12-east-la-r-users/index.html",
    "href": "talks/2018-12-east-la-r-users/index.html",
    "title": "Interactive Dashboards with shiny",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks/2018-09-ucla-cpi/index.html",
    "href": "talks/2018-09-ucla-cpi/index.html",
    "title": "Thoughts and Perspectives on PSH from the Ground Up",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks/2017-09-Data-and-Donuts/index.html",
    "href": "talks/2017-09-Data-and-Donuts/index.html",
    "title": "Ex-nihilo Data Analysis; Getting up and Running with Limited Resources",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "blog/2015-01-first-kaggle-submission/first-kaggle-submission.html",
    "href": "blog/2015-01-first-kaggle-submission/first-kaggle-submission.html",
    "title": "First Kaggle Submission–Random Forest Classifier",
    "section": "",
    "text": "I have seen kaggle mentioned on twitter a lot; mostly by the data scientists and researchers I look up to, but there’s never been much confidence that the site was for me in any way—mostly because I was a long way from my dream data science job with yet so much to learn. Notwithstanding, I cannot help but try and hack my way to my destination! I think it’s a part of my learning process: thrust myself in the midst of something I don’t understand, get stuck, try to get unstuck, finish with some understanding of what I was doing.\n\nSo, when I saw this post by Chris Clark, I thought that it was about time I try and hack my way from recently learning Python to machine learning with SciKit-Learn&—why not!?&—I thought.\n\nIt reminded me of when I decided to sign up with an account at GitHub; I was initially intimidated because it was new to me. Now, I use git in the command line, host my website there, and use it for almost everything (still learning new things about git everyday as well).\n\nChris’s post was excellent but there was one problem: the code was aimed at Python 2.7 users and I had just spent the previous semester learning Python 3 (which means I don’t really know 2.7; and avoid it all the time “where are the parens for this print statement??”). As a personal challenge, I decided to use the code and update it to Python 3, which was both fun and challenging (I’m measuring ‘update’ to mean, ‘running in my Python 3.4 interpreter without error messages’). This may be an easy task but there were a few snags for me.\n\nIn the spirit of trying to document the things I learn, I’ve decided to chronical my results here&—if there are any errors or issues with this code, please let me know so I can try to correct, learn, and grow! I also found Chris’s updated code on GitHub, which uses Pandas and I’ve been trying to get started with Pandas as well so; win, win.\n\nAs an aside, I use Anaconda and Vim for the enviornment and editing, respectively. My code can be found on GitHub.\n\nThe Submission was a part of the Predicting a Biological Response competition, and the training, test, and benchmark data sets are provided.\n\nSince the competition wants us to predict binary values, Chris notes that this data set is a good introduction to ensemble classifiers, because the prediction is a binary value (0 or 1). It was also great to take a closer look at both the Pandas and SciKit-Learn’s documentation to troubleshoot. I tried to use the comments to explain as much as possible so future me will not be baffled, which I can say is helpful since I’m looking at this one month out and it makes total sense (at least to me).\n\n\n### Kaggle Submission Code\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> random forest classifier\n\n    Author: Robertmitchellv\n    Date: Dec 16, 2104\n    Revised: Dec 22, 2014\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef main():\n    # create the training + test sets\n    try:\n        data = pd.read_csv('Data/train.csv')\n    except IOError:\n        print(\"io ERROR-->Could not locate file.\")\n\n    target = data.Activity.values\n\n    train = data.drop('Activity', axis = 1).values\n\n    test = pd.read_csv('Data/test.csv').values\n\n    # create and train the random forest and call it 'rf'\n    # --> n_estimators = the number of trees in this forest, viz.\n    #     100 trees of forest\n    # --> n_jobs set to -1 will use the number of cores present on your system.\n    rf = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n    # fit(X, y[, sample_weight]) = build a forest of tress from the\n    # training set (X, y)\n    rf.fit(train, target)\n\n    # predict_proba(X) predict class probabilities for X as list\n    predicted_probs = [x[1] for x in rf.predict_proba(test)]\n\n    # prep data for use in pd.Series\n    molID, predictProbs = prepData(predicted_probs)\n\n    # use a dictionary with keys as col headers and values as lists pulled from\n    # previous prep function\n    df = {'MoleculeID': molID, 'PredictedProbability': predictProbs}\n\n    # pandas DataFrame = a tabular datastructure like a SQL table\n    predicted_probs = pd.DataFrame(df)\n\n    # write predicted_probs to file with pandas method .to_csv()--add header\n    # for submission\n    try:\n        predicted_probs.to_csv('Data/submission.csv', index = False)\n        print(\"File successfully written; check 'Data' folder\")\n    except IOError:\n        print(\"io ERROR-->Could not write data to file.\")\n\n# preparing data for conversion to pd.DataFrame\ndef prepData(alist):\n        # prepare list to be converted to pandas Series\n        colOne = []\n        colTwo = []\n        idx = 1\n\n        # for loop to set MoleculeID to match the benchmark;\n        # place values into list for easier wrangling as pd.Series\n        for i in alist:\n            colOne.append(idx)\n            colTwo.append(i)\n            idx += 1\n\n        return colOne, colTwo\n\n# call the main function\nmain()\n\n\nAfter performing this–Chris suggested to submit to kaggle; being an extra careful person by nature, I just had to perform the evaluation and cross validation first (I don’t know if any of you feel the same way). Unfortunately, I don’t really understand how the code works–this is one of the problems when hacking through tutorials.\n\n\n### Evaluation/Logloss\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> evaluation function (from Grunthus' post)\n\"\"\"\n\nimport scipy as sp\n\ndef logloss(act, pred):\n    \"\"\" Vectorised computation of logloss \"\"\"\n\n    #cap in official Kaggle implementation,\n    #per forums/t/1576/r-code-for-logloss\n    epsilon = 1e-15\n    pred = sp.maximum(epsilon, pred)\n    pred = sp.minimum(1-epsilon, pred)\n\n    #compute logloss function (vectorised)\n    ll = sum(   act*sp.log(pred) +\n                sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n    ll = ll * -1.0/len(act)\n    return ll\n\n\nThe cross validation was trickier to understand, which I think is mostly due to my not really understanding what ensemble classifiers do, how the random forest classifier works, and more specifically; what training, test, and target data do within machine learning. This gave chase through the SciKit-Learn documentation and other resources online to get a better understanding of what the code was doing&—there’s a lot to learn! The interesting aspect is how the SciKit-Learn reserves some actual data that it can test against the classifier’s predicted values. I tried to show in the comments how I was understanding what the code did at the time.\n\n\n### Cross Validation\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> cross validation\n\"\"\"\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import KFold\nimport numpy as np\nimport pandas as pd\nimport logloss\n\ndef main():\n    #read data from csv; use nparray to create the training + target sets\n    try:\n        train = pd.read_csv('Data/train.csv')\n    except IOError:\n        print(\"io ERROR-->Could not locate file.\")\n\n    target = np.array([x[0] for x in train])\n    train = np.array([x[1:] for x in train])\n\n    # in this case we'll use a random forest, but this could be any classifier\n    model = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n\n    # simple K-Fold cross validation. 10 folds.\n    cv = KFold(n = len(train), n_folds = 10, indices = False)\n\n    #iterate through the training and test cross validation segments and\n    #run the classifier on each one, aggregating the results into a list\n    results = []\n    for traincv, testcv in cv:\n        prob = model.fit(train[traincv], target[traincv]).predict_proba(train[testcv])\n        results.append(logloss.llfun(target[testcv], [x[1] for x in prob]))\n\n    #print out the mean of the cross-validated results\n    print('Results: ', str(np.array(results).mean()))\n\n# call main function\nmain()\n\n\nAfter I was able to execute the submission, logloss, and cross validation code without any errors, I submitted my code to kaggle. It was an exciting moment waiting to see what kind of score I would have recieved had I actually participated in the competition. I would have placed at 325 (well, I would have tied with another user for 325th); check out my results below.\n\n\n\nWell, that wraps up my first submission to kaggle. I really hope this is the first of many. Right now I’m working through the Think Stats + Think Bayes books to refresh my stats knowledge. I’m trying to find time to work on the Titanic tutorial through kaggle as well as perhaps throw a hat in the ring for Booz Hamilton’s Data Science Bowl. There’s so much to learn and I can’t wait for these concepts to become more natural and familiar."
  },
  {
    "objectID": "blog/2015-06-bar-chart-annotations-pandas-mpl/bar-chart-annotations-pandas-mpl.html",
    "href": "blog/2015-06-bar-chart-annotations-pandas-mpl/bar-chart-annotations-pandas-mpl.html",
    "title": "Bar chart annotations with pandas and matplotlib",
    "section": "",
    "text": "When I first started using Pandas, I loved how much easier it was to stick a plot method on a DataFrame or Series to get a better sense of what was going on. However, I was not very impressed with what the plots looked like. Any time I wanted to do something slightly different from the “Plotting” documentation on the pydata site, I found myself arm deep in MPL code that did not make any damn sense to me. This was a problem for me, as I ended up spending way too much time trying to make small edits and not enough time working on the code I was trying to visualize.\n\nOne thing in particular bugged me. I could find no easy to understand tutorial on annotating a bar chart on StackOverflow or any other site. MPL had some documentation, but it was too confusing for me at the time. I spent a lot of time trying to figure out how to put some text right above my bars. Since I would have loved to see a snippet of code to help me in my journey, I thought I would throw it together in a brief post so others could use my workaround.\n\nI warn you, it is not the most elegent solution, I am sure, but it worked for me when I needed to demonstrate the insight I had gained from a Healthcare Access and Utilization Survey (made up mostly of CHIS questions) to people in my department, my director, and her bosses. Since I cannot share any of that data, I will use the War of the Five Kings Dataset that Chris Albon made. I love this data set because I am in the middle of book five of Game of Thrones, which provides a good amount of domain familiarity to enable jumping in easier.\n\n\nSetup + Import Data\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n# set jupyter's max row display\npd.set_option('display.max_row', 1000)\n\n# set jupyter's max column width to 50\npd.set_option('display.max_columns', 50)\n\n# Load the dataset\ndata = pd.read_csv('site_content/data/5kings_battles_v1.csv')\n\n\n\n\nFirst visualization with annotations\n\n\nax = data['region'].value_counts().plot(kind='barh', figsize=(10,7),\n                                        color=\"coral\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Where were the battles fought?\", fontsize=18)\nax.set_xlabel(\"Number of Battles\", fontsize=18);\nax.set_xticks([0, 5, 10, 15, 20])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_width())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()/total)*100, 2))+'%', fontsize=15,\ncolor='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\nThe image above is the output from the Jupyter notebook. I think it is super clear and gives a lot of information about where the battles were fought. However, I am very parital to horizontal bar charts, as I really think they are easier to read, however, I understand that a lot of people would rather see this chart implemented in a regular bar chart. So, here is the code to do that; you will notice that a few things have changed in order to create the annotation.\n\n\nax = data['region'].value_counts().plot(kind='bar', figsize=(10,7),\n                                        color=\"coral\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Where were the battles fought?\", fontsize=18)\nax.set_ylabel(\"Number of Battles\", fontsize=18);\nax.set_yticks([0, 5, 10, 15, 20])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()-.03, i.get_height()+.5, \\\n            str(round((i.get_height()/total)*100, 2))+'%', fontsize=15,\n                color='dimgrey')\n\n\n\n\nI play around with the mpl.text() numbers for almost each chart. They are never exactly where they need to be, which often means moving thigs around a hair here and .03 there. You can add or subtract, which means you can also do this:\n\n\nax = data['attacker_outcome'].value_counts().plot(kind='bar', figsize=(10,7),\n                                                  color=\"indigo\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Do attackers usually win or loose?\", fontsize=18)\nax.set_ylabel(\"Number of Battles\", fontsize=18);\nax.set_yticks([0, 5, 10, 15, 20, 25, 30, 35, 40])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+.12, i.get_height()-3, \\\n            str(round((i.get_height()/total)*100, 2))+'%', fontsize=22,\n                color='white')\n\n\n\n\nIf you are like me, often you like to isolate a categorical value in one column and see what the rest of the dataframe looks like in light of that. It is a simply way of drilling down, but a percentage really would not be as appropriate as a count. Here is an example of using a count rather than a percentage:\n\n\nlosses = data[data['attacker_outcome'].str.contains(\"loss\", na=False)]\n\nax = losses['attacker_king'].value_counts().plot(kind='barh', figsize=(10,7),\n                                                 color=\"slateblue\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Who were the attackers who lost?\", fontsize=18)\nax.set_xlabel(\"Number of Battles\", fontsize=18);\nax.set_xticks([0, 5])\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.1, i.get_y()+.31, \\\n            str(round((i.get_width()), 2)), fontsize=15, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\n\nYou can also just project a couple columns from those that lost to compare a couple of values; I think bar charts are great for this purpose. I am not sure what the best way would be do accomplish this, but here is my implementation:\n\n\nax = losses[['attacker_size', 'defender_size']].plot(kind='bar',\n              figsize=(15,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"For Attacker Losses, What was the Difference in Size?\",\nfontsize=18)\nax.set_ylabel(\"Number of Troops\", fontsize=18);\nax.set_yticks([0, 20000, 40000, 60000, 80000, 100000, 120000, 140000])\nax.set_xticklabels([\"Robb v Joff/Tommen\", \"Joff/Tommen v Robb\", \n                    \"Stannis v Joff/Tommen\", \"Robb v Joff/Tommen\", \n                    \"Stannis v Mance\"], rotation=0, fontsize=11)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+.04, i.get_height()+12000, \\\n            str(round((i.get_height()), 2)), fontsize=11, color='dimgrey',\n                rotation=45)\n\n\n\n\nThere is a handy ‘rotation’ option for the MPL plots that you can use that I feel works well when using a regular bar chart. I really dislike tilting my head to one side to try and read what it says! Also, it is easy to rename the columns! I did not realize how simple it was, which makes me feel silly.\n\nHere is the chart done horizontally, which I prefer:\n\n\nax = losses[['attacker_size', 'defender_size']].plot(kind='barh',\n              figsize=(10,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"For Attacker Losses, What was the Difference in Size?\",\nfontsize=18)\nax.set_xlabel(\"Number of Troops\", fontsize=18)\nax.set_ylabel(\"First Name is Attacker\", fontsize=18)\nax.set_xticks([0, 20000, 40000, 60000, 80000, 100000, 120000, 140000])\nax.set_yticklabels([\"Robb v Joff/Tommen\", \"Joff/Tommen v Robb\", \n                    \"Stannis v Joff/Tommen\", \"Robb v Joff/Tommen\", \n                    \"Stannis v Mance\"])\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+700, i.get_y()+.18, \\\n            str(round((i.get_width()), 2)), fontsize=11, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\n\nI hope this is helpful for anyone out there trying to create little annotations for their visualizations. I feel like this is just a little bit of extra work but it keeps me from having to write JavaScript, which is worth a little copy paste action. When I have time, I would like to create a class with methods so I do not have to keep doing a copy/paste job in my Jupyter notebook.\n\nLet me know if there is an easier way to do this, I would be grateful!\nHere is a link to the notebook on my GitHub if you are interested in playing with it a bit more. I stopped when I was trying to figure out how to turn the dates into a Pandas ‘period_range’."
  },
  {
    "objectID": "blog/2016-08-the-move-to-r/the-move-to-r.html",
    "href": "blog/2016-08-the-move-to-r/the-move-to-r.html",
    "title": "The move to R",
    "section": "",
    "text": "This is not a language wars type post. I do not think there is some Mordor forged language to rule them all. I debated whether or not to even write a post like this. Nevertheless, I had a chance to meet and talk to Jake Powray from DataKind at the DoGoodData confrence and I mentioned my moving from Python to R, which he wanted to hear about since normally he hears about the movement going in the other direction. Since my direction is a bit atypical and since my use case is also different, I decided to write about it. If you have any thoughts feel free to comment below!\n\n\nIt’s been almost a year since I posted anything on this site and quite a bit has changed for me since then. I moved into a more formalized data analyst role at the nonprofit I work at, I developed and put together a survey that relies on around ten research instruments to use as psychometrics (with a longitudinal goal of updating every six months), built a tool to collect the data, tidied the initial 900 responses, performed some clustering that shows a lot of promise, built a Shiny dashboard to communicate results to stakeholders that is being used by programming staff to target their efforts as they work to meet the needs of our respondents, and am gearing up to start bootstrapping the first batch of six month follow-up data to see if any changes are significant. Looking back at my previous posts I am happy to report that I’ve learned a ton—and much of it is due to my picking up and diving into R.\n\nThis isn’t to say that I couldn’t have learned what I did in Python—it just happened to work out in my case that R had a kind of pedagogical effect, which I’ll talk about more in a bit. First, there is no argument that there are insanely smart people who build tools in both languages (some that even collaborate, e.g., feather). Thankfully, I’ve never seen an argument that centers around this theme, i.e., that one language holds the corner on sharp programmers. I do, however, see arguments that appear to be preference masquerading as axiomatic, aphoristic, or general maxim. I’m not sharing some unseen truth; rather, I’m just describing what helped me move forward and gain a better understanding of the data analysis workflow that seems to work for me.\n\nA lot has been said about the steep learning curve of programming in R. By contrast, Python is one of the most popular introductory teaching languges in CS departments at universities across the US. I knew this before I made a choice of which language to learn when I became interested in data analysis. Based on that understanding, I took a Python 3 course at a community college but mostly lived off of the interactive version of How to Think Like A Computer Scientist. Like many people who become interested in data, there is a lot of ground to cover before beginning actual analysis. First, I needed to learn how to program. Second, I needed to learn about computer science. I found the interactive site to be the best way for me to actually get a feel for what happens when I run code and I found Python to be an excellent communicator of general CS ideas. After I finished my Python course and began understanding a bit more about what I was doing in Python I hit the ground running learning Pandas and matplotlib—I thought I had closed the door on R. It didn’t make sense to stop progress in one language to begin all over again in another; especially when people like Hadley and Garret recommend that:\n\n\n However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing.1 \n\n\nI had not mastered Python when deciding to explore R. I have not mastered R either. Nevertheless, there was this nagging thought I couldn’t shake a while back when seeing language-wars-types of posts on twitter about leaving R. Namely, that one necessarily has to use R in order to leave it. Putting aside all the good and bad reasons I’ve read for moving from R to Python, I wondered if R had some sort of pedagogical magic that would help me become a better “data scientist” like the ones I followed on twitter and respected a great deal. In my mind I figured that it must be a cross to bear in order to be a more well rounded data worker.\n\nSince I had spent considerable time figuring out how to annotate bar charts in matplotlib I figured this would be a good test for R: recreate the plots from that blog post to get a feel for what the language is like. To my chagrin, this was much, much, easier in ggplot2 than matplotlib. I felt like I struggled so hard to get matplotlib to give me a halfway decent looking plot while ggplot2 made my previous efforts seem futile. Even adding fit lines with one line of code was a crazy thought to me! I finally understood why Greg Lamp was working so hard to build ggplot in Python. I decided to spend my morning train rides learning more about R and about ggplot2. I didn’t know this at the time, but diving into ggplot2 and its beautiful API offered me insight I always found lacking when getting started with an analysis project in Python. The more I dove into the tidyverse the more I started to see how the API (especially dplyr) unfolds in such a way that a user is practically lead into an analysis workflow; a workflow that I found to be rewarding on many levels.2\n\nBecause of this rewarding relationship, I’ve found a great partner in R—this is especially due to my use case, which I mentioned above and will describe quickly before ending this post. I work as the only person within my org that writes code and works with data the way that I do. Because of this, I often am required to perform many tasks and be very flexible and accommodating of requests. R has proved a great workhorse in this regard; I am able to do so many different things in RStudio that work well for a person that must acomplish everything alone. I can take EDA from an Rmd_notebook, put it in a report, turn it into a webpage, or transform it into a Shiny dashboard. Heck, this whole website was written in RStudio and put together with rmarkdown::render_site(). I know there are many projects I’d like to work on where Python will be the tool of choice. However, at least for now I am dreadfully enamored with R. Python taught me how to think like a computer scientist, but R is teaching me how to think like a statistician. Pedagogically I feel that both languages are vital to growing as the person who works in this hodgepodge known as ‘Data Science’.\n  \n\n\n\n\nFootnotes\n\n\nHadley Wickham and Garrett Grolemund, R For Data Science, http://r4ds.had.co.nz/introduction.html#python-julia-and-friends↩︎\nI know that Hadley has strong opinions about how an analysis workflow should unfold, e.g., “There’s no reason that ifelse() couldn’t be made as fast as your proposed approach, and dplyr will never support in-place mutation of data. This is something I feel very strongly about. Can you provide more examples of the stata code you’re converting? In my experience, stata code tends to use a lot of nested if statements that I’d approach in a fundamentally different way in R.” Read discusion here↩︎"
  },
  {
    "objectID": "blog/2015-04-pandas-snippits/pandas-snippits.html",
    "href": "blog/2015-04-pandas-snippits/pandas-snippits.html",
    "title": "My pandas snippets–always evolving",
    "section": "",
    "text": "The goal of this post is to keep me from googling pandas questions that I’ve forgotten. I don’t know how many times I’ve looked at the results and seen five or more StackOverflow links that have clearly already been clicked on; I feel like Sisyphus when this happens! So, here is what I’m currently committing to memory:\n\n\n### Make matplotlib.pyplot look better with no effort:\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n\n### Delete column\ndel df['colName']\n\n\n### Rename columns\ndf.columns = ['col1', 'col2', 'col3'] # this does not reindex columns\n\n### Combine columns\ndf['newCol'] = df['col1'].map(str) + data['col2'] + data['col3'].astype('str')\n\n### Copy column\ndf['newCol'] = df['oldCol'] # where newCol is the copy\n\n\n### Reindex columns\ncols = ['col1', 'col2', 'col3', 'col4'] # list of how you'd like it\ndf = df.reindex(columns=cols)\n\n\n### Find out how many NaN values you have in a column\ndf['colName'].isnull().sum()\n\n\n### Show unique values\ndf[df['colName'].unique()]\n\n### Create a frequency column from another column\ndf['freq'] = df.groupby('colName')['colName'].transform('count')\n\n### Delete row\ndf = df.drop(2)  # where two is the df's index\ndf = df.drop('rowName')  # if you reindexed\n\n\n### Remove characters before a specific character\ndf['colName'] = df['colName'].apply(lambda x: x.split('-')[-1]) # char = -\n\n\n### Remove characters after a specific character\ndf['colName'] = df['colName'].apply(lambda x: x.split('-')[0]) # char = -\n\n\n### Remove characters, e.g., commas from data\ndf['colName'] = df['colName'].str.replace(',', '')\n\n\n### Convert datatypes, e.g., object to float\ndf[['col4', 'col5', 'col10']] = df[['col4', 'col5', col10]].astype(float)\n\n\n### Convert string date to datetime64\ndf['strDate'] = pd.to_datetime(df['strDate'])\n\n\n### Filter datetime64 column values\nimport datetime\ndf[df['colName'] >= datetime.date(2015, 1, 1)]\n\n\n### Convert NaN values to zeros (or anything else)\ndf = df.fillna(0) # remember that this returns a new object!\n\n\n### Replace string values with numeric representations\ndictionary = {'value1': 1, 'value2': 2, 'Value3': 3}\ndf = df.replace({'colName': dictionary})\n\n\n### Replace multiple cells of a column only with a different string\ndf.loc[df['colName'].str.contains('word'), df['colName']] = \"Different Word\" # or\ndf.loc[df['colA'].str.contains('word'), ['colB']] = 5 # to change a cell in a different column\n\n\n### Project data based on a value range from a column\ndf[df.colWithNumbers <= 360] # shows me values less than or equal to 360\ndf[df['colWithStrings'].str.contains(\"word\")] # shows me values with 'word' in them\n\n\n### Project data based on two values (use and or pipe symbol to denote relationship)\ndf[(df['colWithString'].str.contains(\"word\")) & (df.colWithNumber <= 5)] # and\ndf[(df['colWithString'].str.contains(\"firstWord\")) | (df['colWithString'].str.contains(\"secondWord\"))] # or\n\n\n### Groupby as variable\ngroupedby = df.groupby(df.colName) # or:\ngroupedby = df.groupby(df.colName).add_suffix('/Mean') # add column suffixes\n\n\n### Use groupedby variable and find the mean for your values\ngroupedbyMean = groupedby.mean()"
  },
  {
    "objectID": "blog/2013-09-foucaults-challenge/foucaults-challenge.html",
    "href": "blog/2013-09-foucaults-challenge/foucaults-challenge.html",
    "title": "Foucault’s challenge to modernist classification",
    "section": "",
    "text": "In Foucault’s Les Mots et les choses (The Order of Things), he notes a passage in Borges that, for him, demonstrates the limitations of taxonomic assertions in the face of exotic systems of thought—via Borges, he quotes a ‘certain Chinese encyclopedia’ in which it is written:\n\n\n animals are divided into: (a) belonging to the Emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies1 \n\n\nWhat shocks Foucault about this passage is what connects these categories—i.e., the structure that links these strange juxtaposed oddities: the alphabetical series 2. Foucault questions on what bedrock would kinship between ‘(i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush’-animals meet, other than the non-space of the utterance itself; i.e., the non-space of language 3. Language can only display this kinship in an ‘unthinkable space’ (abstract locus)—Borges wants to remove the ‘operating table’ that enables thought to order, divide, and classify external entities 4. Effectively, this removes the ground upon which, “since the beginning of time, language has intersected space” 5. Borges’ works often lies in the abstract space of the ‘heterotopia’ 6, which “desiccates speech, stops words in their tracks, contests the very possibility of grammar at its source; […] [to] dissolve our myths and sterilize the lyricism of our sentences” 7. This highlights the challenge of classifying in the post-modern library, for now there is an unforeseen danger—not incongruous disorder but the linking together of things that are inappropriate 8.\n\nA perfect classification scheme that represents a universe of knowledge is the pipe-dream of the modernist; our observations are not independent of the external world, which undermines our ability to classify. We are enmeshed in our world; contextualized in the milieu that is the object of our analysis. We have no bird’s eye view—our objectivity has no locus from which to observe. Nevertheless, I feel that there is a space to enhance knowledge organization. First, I believe it is important to shed the illusion of a temporal permanence of facts, which is not solid and more fluid. Meaning, our reconstructions of the external world mirrored by our knowledge organization schemes change through time and are in flux. Second, I believe it is important to increase transparency and acknowledge bias which can exist through ethnocentrism, race, religion, gender, sex, power, language, geography, et cetera.\n\nEffectively this is similar to Jung’s notion of the shadow, that as humans we feel that it is silly to believe we cannot accurately describe the external world—we all agree on things, we test them, and we derive data with which to harness confidence in talking objectively about the external world. We do this daily. It is this confidence that leaves us blind to the shadow of our foundation; like Venice, we are sinking. In order to bring ourselves back to a form of equilibrium we must admit that our shadow exists, namely, that we bring as much to our observations than we leave and, in light of this; we should attempt to root out future bias through honesty and self-understanding. In this way, we stand the chance of building more honest reflections of knowledge. And, again, in this way, we stand the chance of achieving some kind of universe of human knowledge, or at least a good representation of what we feel that we know.\n  \n\n\n\n\nFootnotes\n\n\nFoucault, M. 2002. The order of things an archaeology of the human sciences. London and New York: Routledge Classics (Original work published 1966).↩︎\nIbid↩︎\nIbid↩︎\nIbid↩︎\nIbid↩︎\n“Different from utopias, which also have no locality, heterotopias are disturbing because they undermine language, make impossible the naming of ‘this’ or ‘that’, because they shatter both the syntax that humans construct sentences with, as well as the syntax that holds words and things together”↩︎\nFoucault, 2002↩︎\nIbid↩︎"
  },
  {
    "objectID": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html",
    "href": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html",
    "title": "Advent of code in R: day one",
    "section": "",
    "text": "Over on the rOpenSci Slack, Sam asked if anyone was doing the Advent of Code challenges in R. A few others said they were interested and I decided to go for it as well! My solutions are likely not as savvy as the other more experienced programmers, but it was a fun way to see how other people approach problems and if there is anything about their approach that you can incorporate into your programming style.\nI tend to work often with tibbles and rely often on dplyr so my solution orients itself around a dataframe and using common dplyr functions to solve the problem."
  },
  {
    "objectID": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html#the-problem",
    "href": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html#the-problem",
    "title": "Advent of code in R: day one",
    "section": "The problem",
    "text": "The problem\nThere are two parts to this problem:\n\nPart 1\nThe captcha requires you to review a sequence of digits (your puzzle input) and find the sum of all digits that match the next digit in the list. The list is circular, so the digit after the last digit is the first digit in the list.\nFor example:\n\n1122 produces a sum of 3 (1 + 2) because the first digit (1) matches the second digit and the third digit (2) matches the fourth digit.\n1111 produces 4 because each digit (all 1) matches the next.\n1234 produces 0 because no digit matches the next.\n91212129 produces 9 because the only digit that matches the next one is the last digit, 9.\n\n\nYou’re given data in the form of a long string of integers and task to solve the problem by providing the sum of the digits provided based on the rules introduced above.\n\nHere are the packages I used\nLoad Packages\n\nlibrary(tidyverse)\n\n\nMy puzzle input\n\n\ndata <- c(\"6592822488931338589815525425236818285229555616392928433262436847386544514648645288129834834862363847542262953164877694234514375164927616649264122487182321437459646851966649732474925353281699895326824852555747127547527163197544539468632369858413232684269835288817735678173986264554586412678364433327621627496939956645283712453265255261565511586373551439198276373843771249563722914847255524452675842558622845416218195374459386785618255129831539984559644185369543662821311686162137672168266152494656448824719791398797359326412235723234585539515385352426579831251943911197862994974133738196775618715739412713224837531544346114877971977411275354168752719858889347588136787894798476123335894514342411742111135337286449968879251481449757294167363867119927811513529711239534914119292833111624483472466781475951494348516125474142532923858941279569675445694654355314925386833175795464912974865287564866767924677333599828829875283753669783176288899797691713766199641716546284841387455733132519649365113182432238477673375234793394595435816924453585513973119548841577126141962776649294322189695375451743747581241922657947182232454611837512564776273929815169367899818698892234618847815155578736875295629917247977658723868641411493551796998791839776335793682643551875947346347344695869874564432566956882395424267187552799458352121248147371938943799995158617871393289534789214852747976587432857675156884837634687257363975437535621197887877326295229195663235129213398178282549432599455965759999159247295857366485345759516622427833518837458236123723353817444545271644684925297477149298484753858863551357266259935298184325926848958828192317538375317946457985874965434486829387647425222952585293626473351211161684297351932771462665621764392833122236577353669215833721772482863775629244619639234636853267934895783891823877845198326665728659328729472456175285229681244974389248235457688922179237895954959228638193933854787917647154837695422429184757725387589969781672596568421191236374563718951738499591454571728641951699981615249635314789251239677393251756396\")\n\n\nIt may seem weird, but I thought it would be easier to read this in as a string and then use stringr::str_split() to get each value separated in order to aid in processing.\nSplit the data into a single digit vector\n\ndigits <- data %>% str_split(\"\")\n\n\nMy next idea was to create an index so I don’t have to use a for() loop and rely on the index generated from that process. I also converted the digits back to the integer data type.\nConvert vector to a tibble and add an index\n\npuzzle <- tibble(digits = digits[[1]]) %>% \n  mutate(\n    index = row_number(),\n    digits = parse_integer(digits)) %>% \n  select(index, digits)\n\n\nI find setting up even really simple if_else logic so much easier when using case_when() since I don’t have to worry about the dataframe and can use the variable name. We’re just checking to see if the digit ahead is similar to the digit before and designating the flag in another column with either Match or No Match.\nFind out where the matches are\n\npuzzle <- puzzle %>%\n  mutate(match = case_when(\n    digits == digits[index + 1] ~ \"Match\",\n    TRUE ~ \"No Match\")) \n\nI was convinced for a second I could do something like puzzle$digits[-1] to get the last value but then remembered I was thinking about Python and not R–whoops! Here I’m just checking to see if the first and last digits match since the list is conceptually circular. This reminds me of the first and last lines of Finnegan’s Wake being circular. In any case, this is just a quick check.\nCheck if last and first digits match\n\npuzzle$match[1] <- if_else(puzzle$digits[1] == puzzle$digits[length(puzzle$digits)], \"Match\", \"No Match\")\n\n\nNow we can get the sum and check our work to see if our solution returned a correct response.\nGet the sum\n\npuzzle %>%\n  filter(match == \"Match\") %>%\n  summarise(sum_of_matches = sum(digits))\n\n# A tibble: 1 x 1\n  sum_of_matches\n           <int>\n1           1029\n\n\n\nYAY Correct! 🥂\n\n\n\nPart 2\n\nHere are the rules for part two:\nNow, instead of considering the next digit, it wants you to consider the digit halfway around the circular list. That is, if your list contains 10 items, only include a digit in your sum if the digit 10/2 = 5 steps forward matches it. Fortunately, your list has an even number of elements.\nFor example:\n\n1212 produces 6: the list contains 4 items, and all four digits match the digit 2 items ahead.\n1221 produces 0, because every comparison is between a 1 and a 2.\n123425 produces 4, because both 2s match each other, but no other digit has a match.\n123123 produces 12.\n12131415 produces 4.\n\n\nMy initial thought is to just break the dataframe in half and then check if the digits match, which is acomplished by sliceing it in half and preparing to bind the columns by renaming some variables.\n\nfirst_half <- puzzle %>%\n  slice(1:(nrow(puzzle) / 2)) %>%\n  select(-match) %>%\n  rename(\n    first_index = index,\n    first_digits = digits)\n\nsecond_half <- puzzle %>%\n  slice(((nrow(puzzle) / 2) + 1):nrow(puzzle)) %>%\n  select(-match) %>%\n  rename(\n    second_index = index,\n    second_digits = digits)\n\n\nNow it is a simple bind_cols and then checking for matches, adding matches together and summing that column to get our answer.\n\nfirst_half %>% bind_cols(second_half) %>%\n  mutate(match = if_else(first_digits == second_digits, \"Match\", \"No Match\")) %>%\n  filter(match == \"Match\") %>%\n  mutate(total = first_digits + second_digits) %>%\n  summarise(sum = sum(total))\n\n# A tibble: 1 x 1\n    sum\n  <int>\n1  1220\n\n\nYES! Correct again! 🎉"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "",
    "text": "After seeing many language wars style posts about  vs  and the sort of  to  comparisons being made, I realized that there aren’t many helpful side-by-sides that show you how to do x in y language (and vice versa), I thought about the kind of post I would like to see; one that leverages both tidyverse, modern pandas method-chaining / pyjanitor or polars, and plotly (in both R and Python).\nI decided to try and see if I could contribute something to the discourse. I’m not really trying to reinvent an analysis wheel and just want to focus on the how something is accomplished from one language to the other so I’m pulling from a few sources to just have some code to translate using the same data for both languages.\nSince polars is new to me and I like learning new things, I’m using it for the examples, but if you’re familiar with pandas already, I’d highly recommend pyjanitor."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#data",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#data",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Data",
    "text": "Data\n\nData was obtained from the dslab R package and written to parquet via R’s arrow::write_parquet for better interoporability between R and Python. Additionally, the size is low enough to pull the data as parquet from my GitHub repo."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-r-project-packages",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-r-project-packages",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": " packages",
    "text": "packages\n\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(arrow, include.only = \"read_parquet\")\nlibrary(magrittr, include.only = \"%<>%\")\n\ngapminder <- read_parquet(\"gapminder.parquet\")"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-python-libraries",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-python-libraries",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": " libraries",
    "text": "libraries\n\n\nimport polars as pl\nimport plotly.express as px\n\ngapminder = pl.read_parquet(\"gapminder.parquet\")\n\ngapminder = (gapminder\n  .with_columns([\n    pl.col(\"country\").cast(pl.Utf8),\n    pl.col(\"continent\").cast(pl.Utf8),\n    pl.col(\"region\").cast(pl.Utf8)  \n  ])\n)\n\n\n return top 10 rows in R\n\ngapminder %>% head(10)\n\n\n\n  \n\n\n\n\n get quick info on the data with dplyr::glimpse()\n\ngapminder %>% glimpse()\n\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n\n\n\n return top 10 rows in Python\n\ngapminder.head(10)\n\n\n\n\n\n  \n\n\n\n\n get quick info on the data with pandas’s info DataFrame method\n\ngapminder.to_pandas().info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10545 entries, 0 to 10544\nData columns (total 9 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   country           10545 non-null  object \n 1   year              10545 non-null  int32  \n 2   infant_mortality  9092 non-null   float64\n 3   life_expectancy   10545 non-null  float64\n 4   fertility         10358 non-null  float64\n 5   population        10360 non-null  float64\n 6   gdp               7573 non-null   float64\n 7   continent         10545 non-null  object \n 8   region            10545 non-null  object \ndtypes: float64(5), int32(1), object(3)\nmemory usage: 700.4+ KB\n\n\nThis will come back later, but it’s very easy to move your polars data into a pandas DataFrame."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#hans-roslings-quiz",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#hans-roslings-quiz",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Hans Rosling’s quiz",
    "text": "Hans Rosling’s quiz\n\nFollowing along Hans Rosling’s New Insights on Poverty video, we’re going to answer the questions he poses in connection to child mortality rates in 2015. He asks, which pairs do you think are most similar?\n\nSri Lanka or Turkey\nPoland or South Korea\nMalaysia or Russia\nPakistan or Vietnam\nThailand or South Africa\n\n\n\n\nSri Lanka or Turkey\n\n simple dplyr::filter and dplyr::select\n\ngapminder %>%\n  filter(year == \"2015\", country %in% c(\"Sri Lanka\", \"Turkey\")) %>%\n  select(country, infant_mortality)\n\n\n\n  \n\n\n\n\n simple filter and select method chain\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\"Sri Lanka\", \"Turkey\"]))) \n  .select([\"country\", \"infant_mortality\"])\n) \n\n\n\n\n\n  \n\n\n\n\nThis is where you can start to see how powerful polars can be in terms of the way it handles lazy evaluation. One of the reasons dplyr is so expressive and intuitive (at least in my view) is due in large part to the way it handles lazy evaluation. For people that are tired of constantly needing to refer to the data and column in pandas will likely rejoice at polars.col!\n\n\n\nLet’s just compare them all at once\n\n same strategy; more countries\n\ngapminder %>%\n  filter(\n    year == \"2015\", \n    country %in% c(\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\",\n      \"Malaysia\", \"Russia\", \"Pakistan\", \"Vietnam\",\n      \"Thailand\", \"South Africa\")) %>%\n  select(country, infant_mortality) %>%\n  arrange(desc(infant_mortality))\n\n\n\n  \n\n\n\n\n same as above\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\", \n      \"Poland\", \"South Korea\",\"Malaysia\", \"Russia\", \n      \"Pakistan\", \"Vietnam\", \"Thailand\", \"South Africa\"]))) \n  .select([\"country\", \"infant_mortality\"])\n  .sort(\"infant_mortality\", reverse = True)\n)"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#aggregates",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#aggregates",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Aggregates",
    "text": "Aggregates\n\n grouping and taking an average\n\ngapminder %>%\n  group_by(continent) %>%\n  summarise(mean_life_expectancy = mean(life_expectancy) %>%\n              round(2), .groups = \"keep\")\n\n\n\n  \n\n\n\n\n now with polars\n\n(gapminder\n  .groupby(\"continent\")\n  .agg([\n    (pl.col(\"life_expectancy\")\n        .mean().\n        round(2).\n        alias(\"mean_life_expectancy\"))\n    ])\n  .sort(\"continent\")\n)"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#with-conditionals",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#with-conditionals",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "With conditionals?",
    "text": "With conditionals?\n\n let’s do something slightly more complicated\n\ngapminder %<>% \n  mutate(group = case_when(\n    region %in% c(\n      \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n      \"Northern America\", \n      \"Australia and New Zealand\") ~ \"West\",\n    region %in% c(\n      \"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\n      \"Caribbean\", \"Central America\", \n      \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & \n      region != \"Northern Africa\" ~ \"Sub-Saharan\",\n    TRUE ~ \"Others\"))\n\ngapminder %>% count(group)\n\n\n\n  \n\n\n\n\n rather than use a case_when style function you can continue to chain .when and .then\n\ngapminder = (gapminder.with_columns(\n  pl.when(\n    pl.col(\"region\").is_in([\n      \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n      \"Northern America\", \"Australia and New Zealand\"]))\n    .then(\"West\")\n    .when(\n      pl.col(\"region\").is_in([\n        \"Eastern Asia\", \"South-Eastern Asia\"]))\n    .then(\"East Asia\")\n    .when(\n      pl.col(\"region\").is_in([\n        \"Caribbean\", \"Central America\", \n        \"South America\"]))\n    .then(\"Latin America\")\n    .when(\n      (pl.col(\"continent\") == \"Africa\") & \n      (pl.col(\"region\") != \"Northern Africa\"))\n    .then(\"Sub-Saharan\")\n    .otherwise(\"Other\")\n    .alias(\"group\")\n))\n\n(gapminder\n  .groupby(\"group\")\n  .agg([ pl.count() ])\n  .sort(\"group\")\n)\n\n\n\n\n\n  \n\n\n\n\nI think this is probably a good enough intro to how you’d generally do things. Filtering, aggregating, and doing case_when style workflows are probably the most foundational and this could already get you started in another language without as much headache"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#scatterplots",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#scatterplots",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nI’m trying to strike a balance between dead basic plotly plots and some things you might want to do to make them look a little more the way you want. The great thing about customizing is that you can write functions to do specific things. I don’t want to overload you with defensive programming for custom function writing using Colin Fay’s attempt package, so I’m simplifying a bit; or at least trying to strike a balance. in some instances you can create simple functions or just save a list of values you want to recycle throughout.\n\n + plotly\n\nplotly_title <- function(title, subtitle, ...) {\n  return(\n    list(\n      text = str_glue(\n        \"\n        <b>{title}</b>\n        <sup>{subtitle}</sup>\n        \"),\n      ...))\n}\n\nmargin <- list(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n\ngapminder %>%\n  filter(year == 1962) %>%\n  plot_ly(\n    x = ~fertility, y = ~life_expectancy, \n    color = ~continent, colors = \"Set2\", \n    type = \"scatter\", mode = \"markers\",\n    hoverinfo = \"text\",\n    text = ~str_glue(\n      \"\n      <b>{country}</b><br>\n      Continent: <b>{continent}</b>\n      Fertility: <b>{fertility}</b>\n      Life Expextancy: <b>{life_expectancy}</b>\n      \"),\n    marker = list(\n      size = 7\n    )) %>%\n  layout(\n    margin = margin,\n    title = plotly_title(\n      title = \"Scatterplot\",\n      subtitle = \"Life expextancy by fertility\",\n      x = 0,\n      xref = \"paper\")) %>%\n  config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA quick note about having plotly work inside of the RStudio IDE–as of the time of this writing it isn’t very straightforward, i.e., not officially supported yet. The plot will open in a browser window and it’s fairly snappy. The good think is that on the reticulate side, knitting works! So this side was able to put all this together via rmarkdown when I started this post and Quarto now that I’m finishing this post (remember any  chunk will default to the knitr engine), so that’s pretty cool. We’re even using both renv and pipenv for both environments in the same file \n\n\n\n\n + plotly\n\ndef plotly_title(title, subtitle):\n  return(f\"<b>{title}</b><br><sup>{subtitle}</sup>\")\n\nmargin = dict(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n  \nconfig = {\"displayModeBar\": False}\n\n(px.scatter(\n  (gapminder.filter(pl.col(\"year\") == 1962).to_pandas()),\n  x = \"fertility\", y = \"life_expectancy\", color = \"continent\",\n  hover_name = \"country\",\n  color_discrete_sequence = px.colors.qualitative.Set2,\n  title = plotly_title(\n    title = \"Scatterplot\", \n    subtitle = \"Life expextancy by fertility\"),\n  opacity = .8, \n  template = \"plotly_white\") \n  .update_traces(\n    marker = dict(\n      size = 7))\n  .update_layout(\n    margin = margin)\n).show(config = config) \n\n\n                        \n                                            \n\n\n\nplotly expects a pandas DataFrame so we’re just using .to_pandas() to give it what it wants, but that doesn’t have to stop you from adding any filtering, summarizing, or aggregating before chaining the data into your viz."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#conclusion",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#conclusion",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Conclusion",
    "text": "Conclusion\n\nHopefully this is helpful. If people like posts like this I can try to do more blogging, I just get busy and foregetful sometimes! Feel free to reach out with any feedback or questions."
  },
  {
    "objectID": "blog/2015-10-vaccine-heatmap/vaccine-heatmap.html",
    "href": "blog/2015-10-vaccine-heatmap/vaccine-heatmap.html",
    "title": "Recreating the vaccine heatmap in plotly 4.0 with R",
    "section": "",
    "text": "This took me a while to figure out how to implement. Since plotly 4.0+ is so different1 and the documentation is still rolling out2 I wanted to challenge myself to make something complicated to better understand how things have changed. I think the Impact of Vaccines visualizations are some of the best examples of how powerful data visualization can be–it becomes difficult to mount an counter argument after seeing them. So, here goes!\n\nIn order to get the data for this visualization you will need to create a free account over at Project Tyco. You can download all the datasets if you’d like but I’m just going to use the Polio data set. These are the packages that I’m using:\n\nLoad Packages\n\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nI should probably get in the habit of inspecting the csv files in vim before loading them so I can decide how I want to read in the file. In this case, using the readr package, you’ll need to set a couple of options: first you’ll need to set the skip argument to 2 since there are two lines of metadata about the data set. The second argument is na, which takes a simple vector for weirdly encoded NA values. In this data set, - is used for NA, so you’ll set the na argument to c(\"-\", \"NA\") to ensure those values are handled correctly. The third arugment to set is col_types, which is an awesome way to control your data types during reading so you do less old-fashioned type conversion à la df$x <- as.numeric(df$x), which I don’t think anyone likes doing. readr’s heuristic guesses correctly for all columns except the variables for ALASKA and HAWAII. I’m not exactly sure why, but my guess is that the 0.00 values mess with the NA checks that readr performs. Luckily, we don’t have to know the why since we can just change those two and leave readr to do its job on the others:\n\nRead in the data\n\npolio <- read_csv(\"POLIO_Incidence_1928-1969_20160904215505.csv\", \n                  skip = 2, na = c(\"-\", \"NA\"),\n                  col_types = list(\n                    ALASKA = col_double(),\n                    HAWAII = col_double()\n))\n\n\nNow, the way this data set is structures is super weird. In order to get it into a tidy format we’ll need to use tidyr; specifically the gather function. But I don’t want to pass through all the variable names, so with a little lazy figuring out, I just kept changing the numbers until I found the right range in numbers with dplyr’s select function since it uses the same start_row:end_row notation:\n\nFind the index for the columns you want to gather with tidyr\n\npolio %>% select(3:53) %>% head()\n\n\n\n\n\n\n\n\n\nUse gather to transform data, rename columns, and make sure cases is numeric\n\npolio <- polio %>% gather(3:53, key=\"state\", value=\"cases\") \npolio <- rename(polio, year = YEAR, week = WEEK, state = state, cases = cases)\n\n\nThis is a quick function to get the states data in the right format to use R’s built in state.abb data, which I found online. I’ve never used the match function before, but this seemed to work really well except for DC, which I’m using dplyr’s if_else function to correct. This is a sloppy function but it works for this data.\n\nFix all caps\n\nfix_state_names <- function(data) {\n  lower <- str_to_lower(data)\n  title <- str_to_title(lower)\n  alter <- state.abb[match(title, state.name)]\n  out <- if_else(is.na(alter), \"DC\", alter)\n  return(out)\n}\n\n\nApply the fix_state_names function to the state column\n\npolio$state <- map_chr(polio$state, fix_state_names)\n\n\nWe need to sum all the cases in a given year by state, but if the whole year contains NA values, I’m going to give it a value of 0.\n\ngroup_by to sum all the cases for a year and then ungroup data frame\n\npolio <- polio %>% \n  group_by(year, state) %>%\n  summarise(totals = if_else(all(is.na(cases)), 0, sum(cases, na.rm = T))) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'year'. You can override using the `.groups`\nargument.\n\n\n\nSet margins for plot\n\nm <- list(\n  l = 40,\n  r = 50,\n  b = 50,\n  t = 50,\n  pad = 4\n)\n\n\nYou can create a custom colorscale in a lot of different ways, but I found creating a data frame with tribble to be the easiest way. Trying to replicate this colorscale was difficult because the blues, greens, and yellow colors are only representing values between 0-10% of the total values, so I had to keep changing things until it looked right. Using tribble helps because it is easier to line things up rather than counting vector locations with your finger on the screen (does anyone else do that!?).\n\nCreate custom colorscale\n\npolio_color <- tribble(\n  ~range, ~hex,\n  0.000,  \"#e7f0fa\",\n  0.025,  \"#c9e2f6\",\n  0.045,  \"#95cbee\",\n  0.065,  \"#0099dc\",\n  0.085,  \"#4ab04a\",\n  0.105,  \"#ffd73e\",\n  0.150,  \"#eec73a\",\n  0.300,  \"#e29421\",\n  0.450,  \"#f05336\",\n  1.000,  \"#ce472e\"\n)\n\n\nPlot the data\n\npolio %>%\n  plot_ly(\n    x = ~year, y = ~state,\n    height = 1000, width = 950) %>%\n  add_heatmap(\n    z = ~totals, zmin = 0, zmax = 190,\n    text = ~paste(\n      \"Year: \", year, \"<br>State: \", state,\n      \"<br>Total Cases: \", totals),\n    hoverinfo = \"text\", colorscale = polio_color, showscale = F,\n    opacity = 0.85) %>%\n  add_annotations(\n    \"Vaccine Introduced\", x = 1957.5, y = -1.5, showarrow = F) %>%\n  add_segments(\n    x = 1955, xend = 1955, y = ~state[3], yend = ~state[49],\n    line = list(width = 2, color = \"black\"),\n    name = \"1955\", hoverinfo = \"text\",\n    text = paste(\"Vaccine Introduced: 1955\")) %>%\n  layout(\n    title = \"Polio\",\n    xaxis = list(title = \"\", nticks = 10),\n    yaxis = list(title = \"\", autorange = \"reversed\"),\n    margin = m, autosize = F)\n\n\n\n\n\n\nThere’s a lot going on here, but the code is not too verbose (at least I don’t think). I add the year and state data to the regular plot_ly function while placing the z data that transforms it into a heatmap in add_heatmap. Even though I’m pushing the annotation outside of the plot, plotly doesn’t add any more y ticks. However, in add_segments if I used y = ~min(state), yend = ~max(state) there would have been two extra y ticks on the bottom (0 and 1). This is annoying and I tried many different things to get everything to behave nicely but it’s hard to get a heatmap and lines and annotations to work well together. My workaround is to just make the segment line a little shorter, but I don’t think it looks too bad.\n\nLet me know if you have any thoughts about improving things in the comments!\n  \n\n\n\n\nFootnotes\n\n\nhttp://moderndata.plot.ly/upgrading-to-plotly-4-0-and-above/↩︎\ngit clone and git pull often: https://cpsievert.github.io/plotly_book/↩︎"
  },
  {
    "objectID": "blog/2018-03-uptake-fellowship-start/uptake-fellowship-start.html",
    "href": "blog/2018-03-uptake-fellowship-start/uptake-fellowship-start.html",
    "title": "Reflections on the start of my data fellowship with Uptake",
    "section": "",
    "text": "I can’t believe it’s been more than a week since I returned from my trip to Chicago. This was an incredible opportunity and before I start this post a few thank yous are in order: I really can’t thank Uptake enough for being the kind of company that does more than just ‘have an mission statement’–having the motivation to start a philanthropic arm (the .org side of Uptake) shows the company is serious about the work they intend to do in the sector. This is evident by the data fellows program specifically and I would be remiss if I did not personally thank Andrew Means for designing such a wonderful program in the data for good space. That folks like me miss out on mentoring and growth because we’re often the only data folks in our organization hit very close to home. So, feeling very grateful all around 😄\n\n\n\nPre(R)AMBLE\n\nI always feel that it is important to highlight the path I took to get to the point where I am–the non-traditional route, which begs the question, “what is the traditional route to data science?” Rather than go down a semantics rabbit hole I’d rather point out some of the raw ingredients that, at least I believed when I started down this path, make it easier to get into data science/analysis:\n\n\n\n A good working knowledge of computer science (concepts and practical experience)\n\n\n All the foundational mathematics courses: Algebra to Calculus\n\n\n Some upper division statistics coursework in undergrad and some applied research setting or exposure to experimental design\n\n\n\nWhen I decided that I really wanted to work in this sector this is how I felt:\n\n\n\n❌ I once built an HTML page in the 10th grade about Chrono Trigger locally on a computer I built after the debut of the Pentium III chip so I could finally afford a the Pentium II chip AMA\n\n\n❌ Oh god, mathematics was my least favorite subject in school (tied with orthography)\n\n\n❌ I took intro to stats for my general education requirements but did not really pay attention 😭\n\n\n\nWhen you feel insecure about what you assume other people know it creates a lot of anxiety not only about what you should learn, but also how you should go about learning it and to what level you need learn it in order to have this ‘working knowledge’ of said thing. I’ve been battling these thoughts for almost three years now and even before I flew to Chicago I kept asking myself: am I even ready for an opportunity like this?\n\nHere’s my answer; not just for myself but for anyone else reading this who knows what this feels like: YES 🎉\n\nYES because there is just too much out there for one person to learn and/or know. Jumping into something unfamiliar is sort of like getting married or becoming a parent: you are never ready; the process of becoming is what readies you. So keep it up!\n\n\n\nMy main take aways\n\n\n\n I am actually making progress!\n\n\n My work output as product\n\n\n A little asceticism goes a long way\n\n\n\n\n\nI am actually making progress!\n\nThis was the most encouraging part of the experience–that the past few years of hard work googling things I read on twitter and pouring over blog posts, making my way through Stats and ML books has been working. It’s encouraging to finally feel like you’re at a point where you can really benefit from a mentor, which is a terrific feeling. So much about ML used to go over my head when I jumped into data work with this loony desire to someday become a ‘data scientist and beyond’ that it’s difficult to calibrate when I’m able to say, ‘yes’ I can finally enter the inner chamber of ‘data science’ and really quit the preparation sojourn and begin the rewarding journey ahead. I finally feel like the moment has arrived.\n\nTalking with my other fellows and a handful of the data scientists at Uptake, working through workshop materials, having long conversations about different ML frameworks both within R and outside of the R ecosystem over dinner and on Slack without feeling like I’m totally lost has fueled the 🔥 to keep working harder and to not lose sight of the goal I still have. I am and always plan to be a life-long learner and this is good.\n\n\n\nMy work output as a product\n\nThis was an epiphany. The reports and dashboards and answers to questions that come my way at work is a product. And wearing a product manager hat as a data scientist is a really good skill for me to have–especially when I’m often bridging three domains by myself: engineering, product management, data science. Making sure I’ve really scoped what I intend to do before I start pulling data, munging, modeling, or communicating is something I just wasn’t doing.\n\nThe second part of the epiphany was that there really isn’t anything wrong with a Minimum Viable Product (MVP) because it is the foundation for future iterations. After having the back and forth during a scoping process about what is possible/feasible, making your product effective is not measured in how tight and clean your code is or how complicated the model is; it’s in how well you’ve managed to satisfy the party that needs your expertise. The easiest way to see if you’re on the right track is to provide a MVP early to see how well it does in satisfying what was scoped out. If it doesn’t work and more scoping is required; cool, you didn’t waste any time polishing an artifact destined to the archive heap. If it works then there is ample room/time to really polish the product the way you might like.\n\nThis is my personal downfall as I care wayyy too much about how the things I make look 👀 that I spend wayyy too much time going in a direction that might ultimately be inconsequential to the product’s audience. This is something I am really trying to internalize more.\n\n\n\nA little asceticism goes a long way\n\nMy data science mentor Andrew Hillard said something to me that really hadn’t occurred to me when I was explaining that I wanted to learn to build a Bayesian multi-level model for my project–I wanted to do this in Stan and really get a thorough grounding in this kind of approach. He said (I’m paraphrasing), “one of the things I’ve had to learn here is that this isn’t a Kaggle competition–that often interpretable models like logistic regression perform very well in a surprising amount of use cases. Getting a model to perform 90% of the time is already a success in a lot of ways because going beyond that will take more time and may not operationally improve what the model is predicting is a dramatic way”. This is probably really difficult for those of us who really get excited about using something we just learned to tackle a problem, which I am in no way saying ‘don’t do that’; what I’m saying is that often I have very limited time and in order to try and move the organization into a data driven future, there are many opportunities to deny myself the joy of over-complicating things when it isn’t necessary. This is something I am going to try and keep in mind as I work on my project for the data fellows program and as this process begins to unfold.\n\n\n\nSo what is my project + what are my goals for my program?\n\nI will be writing a separate post about this but here is the high-level highlight:\n\nTo develop deeper machine learning intuition around the kinds of models that maximize interpretability/simplicity with performance in order to drive the reduction in preventable homelessness recidivism by predicting negative housing exits early and to target more supportive services to those residents who are in need. By shifting the focus from ‘Housing First’ to ‘Housing Always’, I’m hoping that I can build good scaffolding through this data for social work professionals to use in developping new ways to diliver programming."
  },
  {
    "objectID": "blog/2014-12-python-journey/python-journey.html",
    "href": "blog/2014-12-python-journey/python-journey.html",
    "title": "The Python journey–One Semester with Python",
    "section": "",
    "text": "This was quite a journey for me. I started the same way everyone else has; with my very first “Hello World” program written in Python 3:\n\n\n# Hello World! program.\ndef main():\n    #get the user's name\n    name = input('What is your name? ')\n    print('Hello World! I am', name)\n\n# Call the main function\nmain()\n\nThis was the first assignment for my Programming in Python course. I was not content to have it only print “Hello World”—no, I need to personalize it in some small way. The following was really interface (if I’m speaking pythonically); to a wider interest in programming qua programming:\n\n\nname = input('What is your name? ')\n\n\nNevertheless, I was not content with this. I allowed myself to be sucked into a forceful vortex that had me thinking I’d be using Jupytr notebooks, matplotlib, etc., to show off how much I know about Python from Twitter. Notwithstanding, the above is what was submitted because I didn’t know how to do any of the fancy stuff I read about. I didn’t know how to use Pandas. I didn’t know how to use Blaze. I didn’t even know how to use ‘conda update conda’ in my terminal (oh; it’s a package manager—not just an easy way to install Python 3.4 on my computer at work without Admin privlages!).\n\nThe reality is that I still have a lot to learn—I’m still in the shallow end. Nothing prepared me for the absolute angst associated with trying to implement a (beginner’s attempt at) the Object Oriented Programming (OOP) paradigm as a final extra-credit assignment! I didn’t even know I had been writing, although very functional; or, very function reliant, procedural code. Somewhere between nesting lists inside of dictionaries, iterating over them, and implementing ‘try, except’ statements, I thought I was really going places with my code. OOP razed that sandcastle quite briskly. Like a kind of soverign and violent natural phenomena.\n\nFrom my first program to my 10th program, this is how far I have come. This is my attempt at OOP, classes, ‘init’ methods, inheritance composition, and more. It’s likely pretty flawed and could be made less redundant, but I didn’t copy StackOverflow and tried to figure it out on my own; so, I’m damn proud of it! There were some programs specs that I needed to show an understanding of; quickly, the program specs:\n\n\nEach question will have four possible answers\nEach player will take turns with the questions\nThere will be a total of 10 questions, each player getting a chance to answer five of them. If the player selects the correct answer, they earn a point. Tell the player whether they got it right or wrong.\nMust create a ‘Question’ class to hold data with the following attributes:\n\nA trivia question\nPossible answer 1\nPossible answer 2\nPossible answer 3\nPossible answer 4\nThe number of the correct answer, e.g., 1, 2, 3, or 4\n\nQuestion class must have an ‘init’ method, accessors, mutators, and a ‘str’ method.\nUse value-returning functions; one named createQuestionsAnswers() that creates the list to display questions and keeps tracks of user input to let players know if they won, lost, or tied.\n\n\nHere are my solutions:\n\n\n# -*- coding: utf-8 -*-\n\"\"\"     A10--Trivia Game!\n        --> two player trivia game\n        --> OOP approach to building the game with classes and objects\n\"\"\"\nimport csv\nimport random\n\n# the Question class acts as a placeholder for the parts of the question\n# needed to construct questions and check answers\nclass Question:\n    # __init__ uses the Data class method getData through composition\n    def __init__(self, question, a1, a2, a3, a4, answer, ansNum):\n        self.getData = Data('csv')\n        self.question = question\n        self.a1 = a1\n        self.a2 = a2\n        self.a3 = a3\n        self.a4 = a4\n        self.answer = answer\n        self.ansNum = ansNum\n\n    # the method performs better as a class method since it instantiates the\n    # Question class with sample questions for the game\n    @classmethod\n    def getQuestion(cls, triviaDict):\n        # using random to get 10 random numbers between a specific range for\n        # trivia questions\n        randomGenerator = random.sample(range(1, 817), 1)\n\n        # for an individual random number in the sample range\n        # --> iterate and use number as index for the trivia questions\n        for i in randomGenerator:\n            question = triviaDict[i][0]\n            a1 = triviaDict[i][1]\n            a2= triviaDict[i][2]\n            a3 = triviaDict[i][3]\n            a4 = triviaDict[i][4]\n            answer = triviaDict[i][5]\n            ansNum = triviaDict[i][6]\n\n        # this creates an instance to return (from question class)\n        aQuestion = Question(question, a1, a2, a3, a4, answer, ansNum)\n        return aQuestion\n\n    # this is a part of using composition rather than inheritance to get the\n    # attributes from the getData method\n    def __getattr__(self, attr):\n        return getattr(self.getData, attr)\n\n    # this method sets up the question (also checks answer)\n    @classmethod\n    def setupAsk(cls, q):\n        print('\\n', q.question, '\\n\\t1: ', q.a1, ' \\\n                 \\n\\t2: ', q.a2, '\\n\\t3: ', q.a3, ' \\\n                 \\n\\t4: ', q.a4, '\\n')\n\n        # make sure the user's input works\n        while True:\n            try:\n                choice = int(input(\"\\nWhat's your answer? \\n--> \"))\n            except ValueError:\n                print('\\nSorry, the answer only accepts numbers; please \\\n                       enter a number 1-4')\n                choice = int(input(\"\\nWhat's your answer? \\n--> \"))\n            finally:\n                if choice in range(1, 5):\n                    break\n\n        # if the question is correct, return true; if not, return false\n        if choice == q.ansNum:\n            print('\\nCorrect! \\n', q.answer)\n            return True\n        elif choice != q.ansNum:\n            print('\\nIncorrect!  \\n', q.answer)\n            return False\n\n# the Data class handles openning the file and preparing it to be used by\n# the Question and Game class\nclass Data:\n    def __init__(self, filetype):\n        self.filetype = filetype\n\n    # opens the CSV to read and prepare it to be used in computaiton later\n    @classmethod\n    def getData(cls):\n        # make sure there isn't an IO error\n        try:\n            # open the csv file + use an index accumulator for dictionary\n            with open('trivia.csv') as csvFile:\n                readCSV = csv.reader(csvFile, delimiter=',')\n                index = 0\n\n                # questions, answer choices, and answers dictionary\n                rowDict = {}\n                questionData = {}\n                # reading the trivia questions\n                for row in readCSV:\n                    rowDict[index] = row\n                    question = rowDict[index][0]\n                    a1 = rowDict[index][1]\n                    a2 = rowDict[index][2]\n                    a3 = rowDict[index][3]\n                    a4 = rowDict[index][4]\n                    answer = rowDict[index][5]\n\n                    # figure out which answer is correct and assign a variable\n                    if answer == a1:\n                        ansNum = 1\n                    elif answer == a2:\n                        ansNum = 2\n                    elif answer == a3:\n                        ansNum = 3\n                    elif answer == a4:\n                        ansNum = 4\n                    else:\n                        print(\"Error!  No correct answer\")\n\n                    # place questions into new dictionary in the right order\n                    questionData[index] = [question, a1, a2, a3, a4, \\\n                                            answer, ansNum]\n                    index += 1\n\n                return questionData\n\n        except IOError:\n            print(\"The file could not be found.\")\n\n# the Game class is where the bulk of the game's structure is found\nclass Game:\n    def __init__(self, playerID, gamePoints):\n        self.playerID = playerID\n        self.gamePoints = gamePoints\n\n    # method to create instances for questions and find out if a quesiton\n    # was answered correctly or not\n    def round(self, qClass, data):\n        gamePoints = 0 # reset to 0 for new round\n\n        # instances\n        q1 = qClass.getQuestion(data)\n        q2 = qClass.getQuestion(data)\n        q3 = qClass.getQuestion(data)\n        q4 = qClass.getQuestion(data)\n        q5 = qClass.getQuestion(data)\n\n        # return value is true or false; this computes points\n        if qClass.setupAsk(q1) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q2) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q3) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q4) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q5) == True:\n            gamePoints += 1\n\n        # let the user know what happenned this round\n        print('you won {} points this game!'.format(gamePoints))\n\n        return gamePoints\n\ndef main():\n    # local variables\n    flag = False\n    gameNum = 1\n\n    # instance of Data class\n    data = Data('csv')\n    questionsData = data.getData()\n\n    # instance of Question class with filler data\n    questions = Question('question', 'a1', 'a2', 'a3', 'a4', \\\n                         'answer', 'ansNum')\n\n    # create both players\n    playerOne = Game(str(input('PLAYER ONE//\\nEnter your name: ')), 0,)\n    playerTwo = Game(str(input('PLAYER TWO//\\nEnter your name: ')), 0,)\n\n    # while loop to keep the game going if the user chooses\n    while flag != True:\n        # let the user know which round they're playing\n        print('\\nROUND ', gameNum, '//\\nPlayer One')\n\n        # first player instance; asks five questions\n        p1round = playerOne.round(questions, questionsData)\n\n        print(\"\"\"\n        +++++++++++++++++++++++++++++++++++++++++++++++++\n        +                   SWITCH PLAYERS!             +\n        +++++++++++++++++++++++++++++++++++++++++++++++++\n        \"\"\")\n\n        # let the user know to switch players\n        print('\\nROUND ', gameNum, '//\\nPlayer Two')\n\n        # second player instance; asks the five quesitons\n        p2round = playerTwo.round(questions, questionsData)\n\n        # let the user know which round they are on with accumulator\n        gameNum += 1\n\n        # figure out who won and use user's inputed name and their points\n        # in print statement\n        if p1round < p2round:\n            print('Thank you for playing!  {} is the winner with {} total \\\n                  game points!'.format(playerTwo.playerID, p2round))\n        elif p2round < p1round:\n            print('Thank you for playing!  {} is the winner with {} total \\\n                  game points!'.format(playerOne.playerID, p1round))\n        elif p2round == p1round:\n            print('There was a tie!  Both {} and {} both earned {} total \\\n                   game points; but you are both still \\\n                   winners!'.format(playerOne.playerID, playerTwo.playerID, \\\n                   p1round))\n\n        # find out if user wants to continue + validate user response\n        while True:\n            try:\n                choice = str(input(\"\\nKeep playing? \\n--> \")).upper()\n            except ValueError:\n                print(\"Sorry, enter either a 'Y' for 'Yes', or 'N' for 'No'.\")\n            finally:\n                if choice == 'N':\n                    flag = True\n                    break\n                if choice == 'Y':\n                    break\n                else:\n                    print(\"please enter either a 'Y' for 'Yes', or 'N' \\\n                           for 'No'.\")\n\n    # say bye to players and quit the program\n    print('\\nThank you for playing!  See you next time!\\n')\n\nmain()\n\n\nI couldn’t help but think about Plato when I was trying to understand how Objects work in Python. There’s something really similar about how a ‘class’ has a kind of ontos—that it isn’t just a blueprint—it exists, and it did exist before the ‘init’ method gave it attributes; that prescriptive human speculation we come up with when describing the form of something abstract like ‘the Beautiful’ (although it’s been a while since my Phil 100A course at UCLA—I hope I’m not misrepresenting the Phaedo).\n\nThe course is over but I have a few titles I purchased from Packt to dig a little deeper. Suggestions are always welcome; the journey’s telos is to learn; and learn I intend to do!"
  },
  {
    "objectID": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html",
    "href": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html",
    "title": "Building this site with RStudio and rmarkdown",
    "section": "",
    "text": "Important\n\n\n\nThis post is outdated; it would still work if you want to go this route, but the current site is built using Quarto and I think it’s a HUGE improvement"
  },
  {
    "objectID": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html#things-that-werent-initially-clear",
    "href": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html#things-that-werent-initially-clear",
    "title": "Building this site with RStudio and rmarkdown",
    "section": "Things that weren’t initially clear",
    "text": "Things that weren’t initially clear\n\nA few things I had to figure out on my own, which I’ll share to make things easier.\n\n\nFooter background color does not extend across the page\n\nThis was tricky and what worked for me was looking at the rmarkdown site’s source code and including two closing <div>’s—here’s how I worked it out in HTML and CSS:\n\nfooter-disqus.html\n\n  </div> <!-- articleBandContent -->\n</div> <!-- pageContent -->\n\n<footer>\n  <div id=\"rbmvFooter\" class=\"footer\">\n    <div class=\"footerContent\">\n    \n    <!-- footer content -->\n    \n    </div>\n  </div>\n  \n  <!-- js functions -->\n\n</footer>\n\n<!-- disqus js for comments -->\n\n\nThis is the CSS I used with the footer above:\n\nstyle.css\n#rbmvFooter {\n  position: relative;\n  z-index: 2;\n  box-sizing: border-box;\n  height: 25em;\n}\n\n#rbmvFooter.footer {\n  background-color: #212121;  /* light black */\n  padding: 4% 8%;\n  margin-top: 7em;\n  box-shadow: 0px 3px 50px 0px rgba(43,45,66,0.4);\n  font-weight: 300;\n  font-size: 13px;\n  line-height: 25px;\n}\n\n#rbmvFooter .footer {\n  color: #626262;  /* darker grey */\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  line-height: 14px;\n}\n\n#scrollTop {\n  color: #FFFFFF!important; /* white */\n}\n\n\n\nSummarising content for the home page\n\nSo this is probably the most painful thing you’ll have to do but it isn’t so bad! Once a post is finished, you’ll have to add the HTML to the index.Rmd file and link to the page. Tools like Jekyll and Pelican handle a lot of this stuff automatically, but it isn’t too bad once you have an idea of how you want it to look. This is what I do:\n\n\nFinish blog post.\nAdd the post and summary / date info to the blog.Rmd file.\nCopy that HTML into the index.Rmd file with the three most up-to-date blog posts.\n\n\nI find it easiest to leave a template in comments that I can just fill in the blanks from the blog post I’ve finished. I don’t think this is too much work. Also, RStudio may come up with some new tooling soon, you never know!\n\nblog.Rmd\n\n<!---\nentries will follow this format:\n<article>\n<h3><a href=\"\"> </a></h3>\n<div class=\"summary\">\n<span><i class=\"fa fa-calendar\"></i> _date_</span>\n<p> ...</p>\n<a class=\"btn btn-outline-primary btn-sm\" href=\"\">Full Post</a>\n</div>\n</article>\n--->\n\n<article>\n<h3><a href=\"blog-the-move-to-r.html\">THE MOVE TO R</a></h3>\n<div class=\"summary\">\n<span><i class=\"fa fa-calendar\"></i> _2016-08-29_</span>\n<p>This is _not_ a language wars type post.  I do not think there is some Mordor forged language to _rule them all_.  I debated whether or not to even write a post like this.  Nevertheless, I had a chance to meet and talk to [Jake Powray](https://twitter.com/jakeporway) from [DataKind](http://www.datakind.org) at the [DoGoodData](http://www.dogooddata.com) conference and I...</p>\n<a class=\"btn btn-outline-primary btn-sm\" href=\"blog-the-move-to-r.html\">Full Post</a>\n</div>\n</article>\n\n\n\n\nAnchor tag in page links missed location\n\nOn my home page I use the smooth scrolling function to move between the summarized content with anchor links and the id attribute. Since I added the fixed class to my _navbar.html file, I forgot to adjust for those pixels. This was fixed with the following CSS:\n\nstyle.css\na.anchor {\n  display: block;\n  position: relative;\n  top: -45px; /* fixed navbar height */\n  visibility: hidden;\n}\n\n\n\nDisplaying CSS in a code chunk\n\nTo get CSS to display in this blog post I had to git checkout gh-pages in the clone of rmarkdown that I made (which I recommend!) to see what they did to display CSS. I mostly use the following format for code chunks: {r, eval=FALSE}, {html, eval=FALSE}, {python, eval=FALSE}, or any of the other engines knitr can handle. {css, eval=FALSE} wouldn’t render for me when writing this post. Instead I had to use the following:\n\n```css\n.someClass {\n  color: #FFFF00;\n}\n```\n\n\n\n\nAlso, how do you put a code chunk in a code chunk?\n\n\n\nSorry, I had to do that. Trying to render the above CSS chunk proved harder than I thought. It lead me to try and figure out how to put a code chunk in a code chunk (and I thought of the ‘yo dawg’ meme right away). Boosted strait from RStudio’s site, this is how you accomplish it if you ever need to know:\n\n\n<pre class=\"markdown\"><code>&#96;&#96;&#96;css\n.someClass {\n  color: #FFFF00;\n}\n&#96;&#96;&#96;\n</code></pre>\n\n\n\n\nGetting Disqus to work\n\nFirst you’ll need to set up an account with Disqus. The instructions are pretty self expanatory for installing Disqus on your page. Once you’ve followed all the instructions, you should be able to navigate to the ‘Installation’ tab, which presents you with options for installing. You’ll need to select “I don’t see my platform listed, install manually with Universal Code”. You’ll be presented with a Jacascript function to place in your site.\n\nI initally didn’t follow Disqus instructions and tried to make it work the way RStudio had it set up in their footer but couldn’t get it to work—what I did was first add the script tag that gives you a count of comments (step 1 under the ‘How to display comment count’) and then add just the function right after (step 1). If you look at the commented out section in the function, you’ll see that Disqus wants you to set two variables and uncomment, but I couldn’t seem to get them to work with my site set up the way it is. If anyone has any insight into how the variables could be set up with an rmarkdown website, let me know in the comments!\n\n\n<!-- disqus -->\n<script id=\"dsq-count-scr\" src=\"//rbmv.disqus.com/count.js\" async></script>\n<div id=\"disqus_thread\" class=\"disqusPadding\"></div>\n  <script>\n    (function() { // DON'T EDIT BELOW THIS LINE\n      var d = document, s = d.createElement('script');\n      s.src = '//rbmv.disqus.com/embed.js';\n      s.setAttribute('data-timestamp', +new Date());\n      (d.head || d.body).appendChild(s);\n    })();\n  </script>\n  <noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n\n\n\n\nSyntax highlighting\n\nI’m not a big fan of the options we inherit from Pandoc. Also, the CSS generated from code chunks seems kind of arbitrary and doesn’t work with Pygments (which is a shame because there are many Pygments CSS files to be found online!). From what I can tell here the markup classes come from the highlighting-kate package. When I looked at the Solarized webpage, there’s a link to the pages css that gave me a bit of a head start. I tweaked it a bit to try and make the chunks look like the syntax highlighting in R, but it isn’t perfect. If you would like to use my highlight theme, just copy this into your CSS:\n\nstyle.css\n\nroot: {\n  --base03: #002b36;  /* background */\n  --base02: #073642;  /* other background */\n  --base01: #586e75;  /* comments / secondary content */\n  --base1: #93a1a1;  /* body text / default code / primary content */\n  --yellow: #b58900;  /* keyword */\n  --orange: #cb4b16;  /* constants */\n  --red: #dc322f;  /* regex, special keywords */\n  --magenta: #d33682;  /* not sure what to do with this color yet */\n  --violet: #6c71c4;  /* not sure what to do with this color yet */\n  --blue: #268bd2;  /* reserved keywords */\n  --cyan: #2aa198;  /* strings, numbers */\n  --green: #859900;  /* operators, other keywords */\n}\n\n/*\n * Solarized Dark \n * http://ethanschoonover.com/solarized\n */\n\npre, code { font-family: 'Roboto Mono', monospace ; font-weight: 300; }\npre { background-color: var(--base03); color: var(--base1); }\ncode.sourceCode .kw { color: var(--blue); font-weight: 700; } /* Keyword */\ncode.sourceCode .dt { color: var(--cyan); } /* DataType */\ncode.sourceCode .dv { color: var(--base01); } /* DecVal */\ncode.sourceCode .bn { color: var(--orange); } /* BaseN */\ncode.sourceCode .fl { color: var(--cyan); } /* Float */\ncode.sourceCode .ch { color: var(--red); } /* Char */\ncode.sourceCode .st { color: var(--green); } /* String */\ncode.sourceCode .co { color: var(--base01); font-style: 100i; } /* Comment */\ncode.sourceCode .ot { color: var(--base1); } /* Other */\ncode.sourceCode .al { color: var(--green); font-weight: 700; } /* Alert */\ncode.sourceCode .fu { color: var(--blue); } /* Function */\ncode.sourceCode .er { color: var(--red); font-weight: 700; } /* Error */\ncode.sourceCode .wa { color: var(--base1); font-weight: 700i; } /* Warning */\ncode.sourceCode .cn { color: var(--orange); } /* Constant */\ncode.sourceCode .sc { color: var(--red); } /* SpecialChar */\ncode.sourceCode .vs { color: var(--cyan); } /* VerbatimString */\ncode.sourceCode .ss { color: var(--cyan); } /* SpecialString */\ncode.sourceCode .im { color: var(--red); } /* Import */\ncode.sourceCode .va { color: var(--blue); } /* Variable */\ncode.sourceCode .cf { color: var(--green); font-weight: 700; } /* ControlFlow */\ncode.sourceCode .op { color: var(--yellow); } /* Operator */\ncode.sourceCode .bu { color: var(--blue); } /* BuiltIn */\ncode.sourceCode .ex { } /* Extension */\n\n\n\nI rand the code chunk above as {html, eval=FALSE} because some of the CSS variables were being classified as .er, which looked really off. I’m still working on the best solution for the syntax highlighting.\n\n\nYou can also go crazy with this and create your own theme that mimics whatever color scheme you are going to use for your site. I thought these colors looked good with my site’s color scheme and the colors I plan to use for data visualizations are still in the works—I will likely change them pretty often.\n\n\n\nPut some ggplot2 in a post!\n\nThe most rewarding part about writing blog posts in rmarkdown is that you can really dive into an analysis workflow and it is automagically a blog post. It’s so easy—you just leave your code chunk as is or pass eval = F if you just want to show some code8 or echo = F to hide or display code the way you want to.\n\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))\n\n# these are the colors I wanted to use for the site; may not be good for viz\nrbmv_palette <- c(\"#BFF073\", \"#0DC9F7\", \"#7F7F7F\", \"#F05B47\", \"#ED1C24\")\n\nmtcars %>%\n  ggplot(aes(x = disp, y = mpg)) +\n    geom_point(color = rbmv_palette[3]) + \n    geom_smooth(formula = y ~ x, se = F, method = \"loess\", \n                color = rbmv_palette[2])\n\n\n\n\n\n\nWhen using ggplot in this way, it creates a directory with an image file to use when creating the HTML. One of the reasons its easier to just use Plotly for web/interactive type stuff.\n\n\n\n\nOk, but how do I add plotly to a page?\n\nI’ve been really into Plotly lately and one of the first things I wanted to do was add it to my website. There are a couple of hoops you have to jump through but luckily there is the wonderful htmltools package to help out9. Oh, and pipes. Plotly loves %>%’s. I’ll just redo the plot above so you get an idea.\n\n\nsuppressPackageStartupMessages(library(plotly))\n\nrbmv_palette <- c(\"#BFF073\", \"#0DC9F7\", \"#7F7F7F\", \"#F05B47\", \"#ED1C24\")\n\nmtcars <- mtcars[order(mtcars$disp), ]\n\nmtcars %>% plot_ly(x = ~disp, y = ~mpg, \n                   type = \"scatter\", mode = \"markers\",\n                   text = ~paste(rownames(mtcars), \"<br>Mpg: \", \n                                 mpg, \"Disp: \", disp), \n                   showlegend = FALSE,\n                   colors = rbmv_palette) %>%\n  add_lines(x = ~disp, y = ~fitted(loess(mpg ~ disp)), \n            mode = \"lines\", name = \"Loess Smoother\", \n            showlegend = TRUE, \n            line = list(color = rbmv_palette[2])) \n\n\n\n\n\n\n\n\nGitHub Pages\n\nYou really don’t need to do a whole lot to host on GitHub. GitHub has really helpful instructions where you can see the two kinds of GitHub pages: a user or organizational account (which is what you’ll use for your blog), or a project page. The only other thing you’ll need to do is creat a .nojekyll file in your repository which you can do quickly in Bash:\n\n\ntouch .nojeckyll\n\n\nIf you’ve purchased a domain and want to use it, you’ll need a CNAME file in your directory as well. This is a really easy file to create:\n\nCNAME\n\nvim CNAME\n\n\nYou are now in Vim, a text editor, and if you’ve never been here will be really confused! So just follow these steps:\n\nPress the i key to ‘insert’, or, type into the text editor.\ntype your url into the file, in my case:\n\n\nrobertmitchellv.com\n\n\nHit the esc key to exit ‘insert’ mode\nType the : key to enter a command (you enter the command at the bottom of the terminal window)\nType ‘wq’ and then hit return\n\nOk, that was sneaky, you can also just create a new file in RStudio (text file) and enter it that way and save it. That will be much easier!\n\n\n\nThat’s it!\n\nThat’s really all I’ve done so far. I just rebuilt this site last week and I’m likely to learn a lot more as I continue but wanted to share what I’ve learned for anyone interested."
  },
  {
    "objectID": "blog/2015-08-p-hacking-and-ontology/p-hacking-and-ontology.html",
    "href": "blog/2015-08-p-hacking-and-ontology/p-hacking-and-ontology.html",
    "title": "P-hacking and ontology",
    "section": "",
    "text": "In a recent FiveTirtyEight post by Christie Aschwanden about researcher bias and P-Hacking, there is a lovely interactive example of what variables a researcher would need include/exclude in the analysis in order to obtain a result that is statistically significant, i.e., p≤.05; thus worthy of publishing. The article brought many thoughts to mind, which I am using this blog post to note. The crux of the article, I feel, rests in the following passage from the article:\n\n\nThe important lesson here is that a single analysis is not sufficient to find a definitive answer. Every result is a temporary truth, one that’s subject to change when someone else comes along to build, test and analyze anew.1\n\n\nThis annotates the concept of truth; introducing time and the transitory nature of what we know as ‘truth’. However, if we can think about what it means for some subject to be at one point in time and to be at some other point in time, this, again, introduces the concept of ontology; i.e., what does it mean for a subject to exist? There is a very novel explanation from Joseph Tennis that describes how both time and ontology interact that I would like to introduce:\n\nKnowledge changes through time. Classification schemes as tools for accessing knowledge undergo constant revision. It is impossible to claim that the ontology of subjects and their interrelationships, once established by a classificationist, remain constant within that scheme. As revisions to classification schemes emerge, so too do new subjects. These, new parts of the updated classification scheme are elements in a formal system—elements that represent the current interpretation of knowledge.2\n\n\nThis is both simple and profound. Time is an interesting concept and it has, for the most part, been missing from the philosophical discourse from Heraclitus to Heidegger.3 If we can remember from the Heraclitus Fragments, “You cannot step twice into the same stream. For as you are stepping in, other waters are ever flowing on to you”.4 This statement is perhaps the most profound philosophical aphorism I have encountered since I began to love learning. It decimates our ability to truly understand things metaphysically.5 Nevertheless, it lets you peek, at least in part, at something beyond our anthropocentric naive empiricism. This is important, because there is rich meaning in the way subjects change over time—it can tell us a lot about ourselves. There is another passage I like in Tennis’ paper that I’d like to return to:\n\n\n What kind of access is granted by a classification system that shows how knowledge has changed, verses one that revises classes, denying access to the classificationist’s interpretation of the change in knowledge? With each revision, a scheme for classification cuts itself off from its previous view of knowledge, building an artificial boundary of time. There are other rhetorical questions pertinent to time as it relates to subject access. For example, could one access the array of subjects in higher education that were taught during Plato’s Greece? Through a classification scheme, can one collocate the works of proto-anthropologists? These knowledges are not reflected in classification schemes, because each living scheme needs to be revised to be viable—thereby eliminating the fossil record of literary warrant. To what degree do revised classification schemes blind us to how subjects change and are re-collocated through time? What can knowledge organization thery do to help the sophisticated user re-collocate knowledge through time? This can be answered by charting the development of a class in a classification system through time. In other words, this can be answered by charting the subject’s ontogeny.6\n\n\nThis really reminds me a lot of Heidegger’s On the Origin of the Work of Art and both his concept of “world” and the “work of art”, which has its own poetic interaction. A former professor of mine at UCLA named John McCumber helped shape the way I think philosophically. He was one of the most intimidating professors I had; able to recall from memory huge passages of Hegel, Heidegger, and Nietzsche in German, fluent in French, and writing ancient Greek and Latin on the whiteboard when explaining concepts. In his book Poetic Interaction: Language, Freedom, Reason, there is a passage I think ties some of these floating ideas together:\n\n\n[P]reserving the work of art functions similarly to Being-toward death in Being and Time. Heidegger in fact goes on to relate the preservation of the work of art to the concept of “resoluteness” presented in the book. But the difference is unmistakeable. The inarticulateness of resolve in Being and Time is replaced with the concrete individuality of the work of art, which speaks to us, not from within our world or as an indeterminate “call” out of it, but from another concrete world, one unique to itself. This way of experiencing a work of art is a condition for its being a work of art at all. An art work which does not deserve an audience, we must say, is no art work. This is why, in experiencing a work of art for what it is, we “preserve” it; and it is why, for Heidegger as for Hegel, the work of art is intrinsically a communal and (in a broad sense) a communicative entity.7\n\n\nThis may be confusing at first, because the quote is taken a bit out of context, but the phrase that captivates me is the section where McCumber talks of the work of art being preserved within its own world. Ontologically this is fascinating because the work of art has aged—it is not as it was when created. Moreover, it is not necessarily a part of this world either; it is preserved in some capacity. My main interest in subject ontogeny is the attemp to make explicit the effect time has on our body of knowledge. Being able to understand what encyclopedic resources were available when Plato was alive is a form of artistic expression because it crafts an experience that people can interact with—it would preserve Plato’s world in a small sense and that world would rest apart from our own world. That we could be transported is the exciting notion; that we could stretch our conceptions, biases, cultural stereotypes, all the mental baggage we carry around in our mind from being thrown into the world at this particular time in its existence—this would provide perspective that moves our species forward.\n\nSo, what does this have to do with P-Hacking? It is that truth is temporary. Results, data, these things are not stateless; they say as much about this time that we are doing research as they do informing us of new temporary truths. We could argue about what is currently axiomatic and how certain certainties are foundational, but this is only an affirmation of a kind of thinking that needs the external world to conform to our senses and instruments to make meaning. As humans we are always making meaning and mistakes—those things go hand in hand. Like biological life, knowledge is iterative and it evolves. As our knowledge evolves, so too should our way of describing that evolution. The philosophical notion of ontology can be therefore connection to the information science concept of ontology8 if we are able to represent human knowledge’s interrelationships that describe what we know, temporally, to be true for all domains of discourse in as many ways as can be accurately and approximately completed. Such an ontology would reflect in an abstract and concrete way what it is to be human. What would tell us who we are.\n\nScience is a bit of a dialectic—there is movement in going from an unknown to a known that has a telos that can and should be represented in an ontology. Therefore, to say that we know something is to make an epistemological statement, which begs an epistemological question: how do you know that? When our understanding of science is as Aschwanden says:\n\nScience is not a magic wand that turns everything it touches to truth. Instead, “science operates as a procedure of uncertainty reduction […] [t]he goal is to get less wrong over time.” This concept is fundamental—whatever we know now is only our best approximation of the truth. We can never presume to know everything.9\n\nThat this line of thinking is novel for people in STEM fields is as disconcerting as the lack of statistical knowledge in the humanities. I see this as perhaps the most pervasive mistake in higher education. On the one hand, we are told not to be a generalist; to specialize, to find a niche and exploit it. On the other hand, we miss out on perspectives, ways of thinking, and challenges that provoke deeper thinking than would normally be the case. In working toward teaching myself computer science and mathematics, I am continually pushing myself to broaden my understanding of the world. How are STEM only researchers doing the same? Without a complete education that covers all domains at least in part we lack the tools to truly understand the 21st century. We need interdisciplinary teams of interdisciplinarians; not callow one-faceted domain caricatures that cannot think in multiple loci. I believe this will enable humanity to probe deeper into complexity and how ecologically interdependent we all are on Earth.\n  \n\n\n\n\nFootnotes\n\n\nAschwanden, C. (2015, August 19). Science Isn’t Broken: It’s just a hell of a lot harder than we give it credit for. Retrieved August 25, 2015, from https://fivethirtyeight.com/features/science-isnt-broken/↩︎\nTennis, J. T. (2002). “Subject Ontogeny: Subject Access through Time and the Dimensionality of Classification.” In Challenges in Knowledge Representation and Organization for the 21st Century: Integration of Knowledge across Boundaries: Proceedings of the Seventh International ISKO Confrence. (Granada, Spain, July 10-13, 2002). Advances in Knowledge Organization, vol. 8. Wurzburg: Ergon: 54-59.↩︎\nI inherited this opinion from Dr. John McCumber through his various philosophy courses and seminars. See http://www.germanic.ucla.edu/people/faculty/mccumber/ for more info.↩︎\nhttps://en.wikiquote.org/wiki/Heraclitus↩︎\nWhat I mean is that, when reading the passage, we are in the river; the thing we are trying to classify—to understand. The fact that we have no bird’s eye view is an important caveat to working toward understanding the world in a realistic way. We call it one thing, but it is something else outside of the way humans communicate it to each other through language.↩︎\nTennis (2002)↩︎\nMcCumber, J. (1989). Poetic interaction: Language, freedom, reason. Chicaco: University of Chicago Press↩︎\nhttps://en.wikipedia.org/wiki/Ontology_(information_science)↩︎\nAschwanden (2015)↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "robertmitchellv",
    "section": "",
    "text": "A tidyverse R and polars Python side-by-side\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nReflections on the start of my data fellowship with Uptake\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nAdvent of code in R: day one\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2017\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nBuilding this site with RStudio and rmarkdown\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThe move to R\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2016\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nRecreating the vaccine heatmap in plotly 4.0 with R\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nP-hacking and ontology\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nBar chart annotations with pandas and matplotlib\n\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nMy pandas snippets–always evolving\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nFirst Kaggle Submission–Random Forest Classifier\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThe Python journey–One Semester with Python\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2014\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nFoucault’s challenge to modernist classification\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2013\n\n\nRobert Mitchell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "robertmitchellv",
    "section": "",
    "text": "LA County COVID-19 Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2020\n\n\nRobert Mitchell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professionally\nMy background is in theology, philosophy, and continental thought. This formed the basis for my academic exploration. My BA was in both French and comparative literature at UCLA. In the US, the ‘comp lit’ field is dominated by literary criticism and the search for universal truths and archetypes in global literature, which is not always apparent from the field’s name. Additionally, I took a fair amount of philosophy courses (my major originally), German language courses, and a bit of Scandinavian literature, film, and Old Norse (inspired from a trip to Iceland).\n\nMy Master’s degree is in Information Science (MSIS). Specifically, Information Systems. The MSIS is designed for information work in many different fields and is a bit of an odd degree for someone with my work experience. The information systems track taught me how to support my organization by thinking systematically and procedurally; and I combine that with my background in critical theory to ask why some formalization exists–and after all the facts are gathered–whether or not there is a better way to operationalize something.\n\nProfessionally I’ve gone from being the sole data person doing, what I suppose we could call full stack data science to leading a data science team within local government. Given the limited resources of nonprofits and local gov, I have a lot of experience finding a way to get things off the ground without a ton of resources. The longer I work in roles like this the more I end up drifting more and more into DevOps territory. As a leader, I want to figure out the best way for my team to work and often that means figuring out the infrastructure so they don’t have to. I do my best to create an environment that allows them to do as much dev work with the least amount of distractions as possible.\n\n\n\nPersonally\nI’ve been lucky enough to do a fair amount of traveling; a few months in Japan at 19, a year in Paris at 23/24, Summer in Germany and Scandinavia at 24, and some quick trips to different places while spending quality time in these locations. These experiences have shaped who I am as an individual, before I worked up the courage to talk to a girl sitting across from me on the Metro. This changed everything. I went from an individual to a couple to a family in what felt like no time at all.\n\nI hope to contribute posts/projects at the intersection of my various interests. I would like to have more time to explore the semantic relationships between objects and concepts in time. Also, I can never seem to escape the epistemological underpinnings of things and often spend time thinking about how probability and statistics is grounded. Deep down I am fairly optimistic: I believe that everything is connected and that the 21st century will be about discovering these connections—-hopefully I can be a part of this. At least in some small way."
  },
  {
    "objectID": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "href": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "title": "robertmitchellv",
    "section": " hello there! this is a website I don’t always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers ",
    "text": "hello there! this is a website I don’t always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "robertmitchellv",
    "section": "",
    "text": "Interactive Dashboards with shiny\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThoughts and Perspectives on PSH from the Ground Up\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nEx-nihilo Data Analysis; Getting up and Running with Limited Resources\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2017\n\n\nRobert Mitchell\n\n\n\n\n\n\nNo matching items"
  }
]