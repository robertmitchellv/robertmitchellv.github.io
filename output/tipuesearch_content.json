{"pages":[{"url":"http://robertmitchellv.com/about-me.html","text":"The brief bio I am father, husband, and recent Master of Science in Information Science (MSIS) recipient who is passionate about code, complexity, design, dichotomy busting, cuisine, travel, + striving for balance across the board in my life. I love conceptual thinking and am expanding my worldview through the acquisition of knowledge. With this site, I hope to document what I learn and share what I know. Still curious? Read on My background is rooted in theology, philosophy, and continental thought. This formed the basis for my academic exploration. My BA was in both French and comparative literature at UCLA. In the US, the ‘comp lit' field is dominated by literary criticism + the search for universal truths and archetypes in global literature, which is not always apparent from the field's name. Additionally, I took a fair amount of philosophy courses (my major originally), German language, literature, art, and philosophy courses with a bit of Scandinavian literature + film courses and a couple courses on dead languages here and there (Old Norse was particularly interesting to study). My Master's degree is in Information Science (MSIS). Furthermore, I also completed both the Information Systems program of study; a narrower focus within the MSIS degree as well as a Graduate Academic Certificate (GAC) in Digital Content Management. The MSIS is designed for information work in many different fields from information brokerage, database analytics, chief information officer, user experience designer, information architect, knowledge manager, system librarian, and many more. The information systems track taught me how to support my organization to gain strategic and tactical competitive advantages, which we can call knowledge management or business intelligence. It supports more technical knowledge and builds more technical skills than the other tracks within the MSIS. Last, the GAC in Digital Content Management provided me with a theoretical foundation + conceptual toolkit to help me manage digital content, build applications, and develop services that respond to institutional + individual user needs. I've been lucky enough to do a fair amount of traveling; a few months in Japan at 19, a year in Paris at 23/24, summer in Germany and Scandinavia at 24, and some quick trips to different places while spending quality time in these locations. These experiences have shaped who I am as an individual, before I hit on a girl sitting across from me on the Metro, eventually marrying her (best impulsive move ever). I hope to contribute posts on my site that interact with my interdisciplinary interests with special focus on the semantic relationships between objects and concepts. I'm not always as interested in the epistemological aspect of things as I might appear–really I think I enjoy metaphysical speculation (otherwise, why would I have become interested in philosophy at all?). I believe that everything is connected, and the 21st century will be about discovering these connections—hopefully I can be a part of this. Go Back!","title":"About Me","tags":"pages"},{"url":"http://robertmitchellv.com/connect.html","text":"Some of the places and online spaces you can find me: | Twitter | GitHub | LinkedIn | Soundcloud Feel free to connect with me via the above referenced outlets! Go Back!","title":"Connect","tags":"pages"},{"url":"http://robertmitchellv.com/vitae.html","text":"Personal Philosophy I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges—process + task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation— working toward something with the shared value of doing good. I bring a lot of energy + enthusiasm to the projects that I become involved in. I thrive in situations that require imagination + innovation. I have a broad range of interests; from academic inquiry, classical and contemporary thought, the music + cuisine of other cultures, the arts, travelling, languages; to a passion for social justice, education for all people, + that our most vulnerable deserve respect + to be treated with dignity + fairness Ultimately I hope that the work I do is helpfult—that it enables discovery or provides clarity. The gamut of which is not narrow, but wide—a lot can be helpful or meaningful. Things practical and principled. I hope to further grow as this philosophy guides me. Personal Skills I am dedicated + hard working—a good communicator + active community participant. I work well on team efforts + projects with an eye toward outcomes I am: creative honest friendly enthusiastic keen to learn new things + take on new roles reliable a lateral thinking motivated I am able to: speak + read French read + understand German listen to + follow instructions accurately work cooperatively take on new challenges complete tasks support others identify new innovations + technologies while working toward implementing them be proactive Technical Skills Web Technologies: HTML, CSS, Javascript, Flask, Pelican, Jekyll, Wordpress, Drupal, Information Architecture, + UX/UI design. Coding: Python, R, + some Javascript. Would like to learn something functional next; thinking Clojure! OSs: OS X, Linux, + Windows (cygwin!). I love the terminal and wish vim bindings were universal. Data: Knolwedge of Relational Databases, SQL, knowledge organization theory, + currently interested in learning noSQL through MongoDB. Data Analysis: (python) munging and basic stats with Pandas, mostly use matplotlib with the ggplot style (a must) within Pandas. Also, I have some basic scikit learn clustering exposure and want to get into PYMC but there is still so much to learn! (r) mostly basics in dplr, and ggplot2 (hadleyverse). Metadata schemas: knowledge of AACR2, RDA, MODS, METS, + PREMIS Education University of North Texas | Master of Science in Information Science (2013) University of California, Los Angeles (UCLA) | Bachelor of Arts in French + comparative literature (2008 / double major) Université de la Sorbonne Nouvelle (Paris III) | Student exchange (2006—2007) Work Experience Skid Row Housing Trust_Los Angeles, CA Data Analyst / August 2015—Present provide strategic oversight to organization's data collection systems and reporting processes through the collection and anslysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture responsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming exploring open-source database alternatives to reduce organizational cost and fit within the IT ecosystem liaise with internal departments for reporting/proposal questions, demographic data, and to answer questions about the Resident Programs department's model and outcomes strategy prepare recurring and special aggregated data reports and related decision support tools for Resident Programs department (and organization if needed) to support analytics value chain: data reporting analysis action value oversee the application and implementation of assessment procedures; provides feedback and training in its use and answers questions about what is tracked and why gathers, formats, and analyzes data using descriptive and inferential statics with visualizations of outcome metrics to enable the organization to tell its narrative with data (in Python) acting as core Efforts to Outcomes database administrator for the Resident Programs department; provides additional database support / programming support to internal departments as requested working on data warehousing solutions to support executive dashbord to track high-level concerns liaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with residents to answer questions about what we currently are using to track outcomes / offer support Skid Row Housing Trust_Los Angeles, CA Health Services Coordinator / October 2014—July 2015 gathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake. performed statistical analysis of health survey data using Python--we had 200 respondants and were able to make programmatic changes based on our findings developed and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions implemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents planed and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community provided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and wellbeing prepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database project troubleshooter par excellence. Skid Row Housing Trust_Los Angeles, CA Housing Transition Specialist / November 2013—September 2014 responsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining condidentiality and records keeping due diligence worked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA researched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project point-person/project manager for follow up and grant workplan compliance with Skid Row Community Consortium researched and identfied future survey instruments for targeted subpopulations to improve data collection practices participated in regular trainings, program evaluation, and program development designed and edited Peer Advocate zine for commercial printing improved client access to resources/information through small booklets tailored to the population UCLA Powell Library_Los Angeles, CA Social Media Intern / January 2013—November 2013 designed + implemented new digital signage/print flyers to promote library outreach, services, + events created new social media presence for (then) new platform Vine to promote library collections + services designed online content that highlighted collections, incorporated teaching + learning services + provided wide audience appreciation facilitated presentation on approach + ideas to UCLA Library's social media steering committee operated multi-tiered social media platform apparatus LAC Group | on Assignment for USC Library_Los Angeles, CA Library Clerk / June 2013 searched bibliographic records in ILS + assisted with processing + discarding of duplicates performed copy cataloging of print monographs + serials willingness to take on new/unfamiliar tasks in order to meet project contract requirements maintained strong attention to detail Taylor Mortgage Lawyers_Pasadena, CA Administrator / May 2010—February 2011 drafted outgoing legal correspondence + created document templates to ease workflow managed + trained receptionists on tasks, etiquette, + computer/office equipment communicated effectively with diverse groups acted as firm reference point for difficult or challenging matters immense use of research skills, ease in digesting difficult subjects/literature, teamwork, + timeliness Chino Valley Unified School District_Chino, CA Substitute Teacher / September 2008—July 2009 facilitated classroom instruction for wide socio-economic populations + age groups able to pull together resources on the fly + improvise in the absence of a lesson plan passion to teach students useful skills for successful academic career UCLA Philosophy Department_Los Angeles, CA Library Assistant / January 2006—August 2006 responsible for the circulation + shelving of philosophy department's library materials assisted with document delivery for department researchers and faculty carefully handled, cataloged (rudimentary metadata), + shelved donated library materials from estates of previous emeritus professors; chiefly Dr. Rudolf Carnap Accomplishments Designed, proofed, prepared, + labored over Peer Advocate 'Zine for Skid Row Housing Trust's Hilton Project 'Zine \"I Got You\", which will be released during a end of grant event for all project participants Was asked to help design + brainstorm a flyer for the UCLA Library system because of interest in design + outreach for libraries—the final project was the \"Top 10 Things\" flyer, which was a successful campaign to engage students in library programming + services Was recognized by two articles online for work promoting library services + collections through early-adoption of Vine Interests Learning to code; currently, I'm in my second semester of CS courses and am taking a Javascript course and a Linux course. Additionally, I am enrolled in a linear algebra course through Coursera as well as a discrete mathematics course through edX. I try to challenge my knoweldge as much as possible in order to fill in knowledge gaps for advanced computer science/computation programs (hopefully I'll be moving to Europe next fall!). Beverages: third-wave coffee shops/roasts, home-brewing on V60, Chemex, + Yama Siphon; natural wine + out of the ordinary grapes, flavors, + regions; beers with lightly roasted malt--especially aged, or spontaneously fermented (read: sour!) Design: I love typefaces + modernism; as you can see from my portfolio, it is something I like to do when time permits Exploration: I like to explore other places through travel + food; ideologies through reading + communicating, + world through science + discovery! Go Back!","title":"Vitae","tags":"pages"},{"url":"http://robertmitchellv.com/blog/p-hacking-and-ontology.html","text":"In a recent FiveTirtyEight post by Christie Aschwanden about researcher bias and P-Hacking, there is a lovely interactive example of what variables a researcher would need include/exclude in the analysis in order to obtain a result that is statistically significant, i.e., p≤.05 ; thus worthy of publishing. The article brought many thoughts to mind, which I am using this blog post to note. The crux of the article, I feel, rests in the following passage from the article: The important lesson here is that a single analysis is not sufficient to find a definitive answer. Every result is a temporary truth, one that's subject to change when someone else comes along to build, test and analyze anew. 1 This annotates the concept of truth; introducing time and the transitory nature of what we know as 'truth'. However, if we can think about what it means for some subject to be at one point in time and to be at some other point in time, this, again, introduces the concept of ontology; i.e., what does it mean for a subject to exist? There is a very novel explanation from Joseph Tennis that describes how both time and ontology interact that I would like to introduce: Knowledge changes through time. Classification schemes as tools for accessing knowledge undergo constant revision. It is impossible to claim that the ontology of subjects and their interrelationships, once established by a classificationist, remain constant within that scheme. As revisions to classification schemes emerge, so too do new subjects. These, new parts of the updated classification scheme are elements in a formal system—elements that represent the current interpretation of knowledge. 2 This is both simple and profound. Time is an interesting concept and it has, for the most part, been missing from the philosophical discourse from Heraclitus to Heidegger. 3 If we can remember from the Heraclitus Fragments , \"You cannot step twice into the same stream. For as you are stepping in, other waters are ever flowing on to you\". 4 This statement is perhaps the most profound philosophical aphorism I have encountered since I began to love learning. It decimates our ability to truly understand things metaphysically. 5 Nevertheless, it lets you peek, at least in part, at something beyond our anthropocentric naive empiricism. This is important, because there is rich meaning in the way subjects change over time—it can tell us a lot about ourselves. There is another passage I like in Tennis' paper that I'd like to return to: What kind of access is granted by a classification system that shows how knowledge has changed, verses one that revises classes, denying access to the classificationist's interpretation of the change in knowledge? With each revision, a scheme for classification cuts itself off from its previous view of knowledge, building an artificial boundary of time. There are other rhetorical questions pertinent to time as it relates to subject access. For example, could one access the array of subjects in higher education that were taught during Plato's Greece? Through a classification scheme, can one collocate the works of proto-anthropologists? These knowledges are not reflected in classification schemes, because each living scheme needs to be revised to be viable—thereby eliminating the fossil record of literary warrant. To what degree do revised classification schemes blind us to how subjects change and are re-collocated through time? What can knowledge organization thery do to help the sophisticated user re-collocate knowledge through time? This can be answered by charting the development of a class in a classification system through time. In other words, this can be answered by charting the subject's ontogeny. 6 This really reminds me a lot of Heidegger's On the Origin of the Work of Art and both his concept of \"world\" and the \"work of art\", which has its own poetic interaction. A former professor of mine at UCLA named John McCumber helped shape the way I think philosophically. He was one of the most intimidating professors I had; able to recall from memory huge passages of Hegel, Heidegger, and Nietzsche in German, fluent in French, and writing ancient Greek and Latin on the whiteboard when explaining concepts. In his book Poetic Interaction: Language, Freedom, Reason , there is a passage I think ties some of these floating ideas together: [P]reserving the work of art functions similarly to Being-toward death in Being and Time . Heidegger in fact goes on to relate the preservation of the work of art to the concept of \"resoluteness\" presented in the book. But the difference is unmistakeable. The inarticulateness of resolve in Being and Time is replaced with the concrete individuality of the work of art, which speaks to us, not from within our world or as an indeterminate \"call\" out of it, but from another concrete world, one unique to itself. This way of experiencing a work of art is a condition for its being a work of art at all. An art work which does not deserve an audience, we must say, is no art work. This is why, in experiencing a work of art for what it is, we \"preserve\" it; and it is why, for Heidegger as for Hegel, the work of art is intrinsically a communal and (in a broad sense) a communicative entity. 7 This may be confusing at first, because the quote is taken a bit out of context, but the phrase that captivates me is the section where McCumber talks of the work of art being preserved within its own world. Ontologically this is fascinating because the work of art has aged—it is not as it was when created. Moreover, it is not necessarily a part of this world either; it is preserved in some capacity. My main interest in subject ontogeny is the attemp to make explicit the effect time has on our body of knowledge. Being able to understand what encyclopedic resources were available when Plato was alive is a form of artistic expression because it crafts an experience that people can interact with—it would preserve Plato's world in a small sense and that world would rest apart from our own world. That we could be transported is the exciting notion; that we could stretch our conceptions, biases, cultural stereotypes, all the mental baggage we carry around in our mind from being thrown into the world at this particular time in its existence—this would provide perspective that moves our species forward. So, what does this have to do with P-Hacking? It is that truth is temporary. Results, data, these things are not stateless; they say as much about this time that we are doing research as they do informing us of new temporary truths. We could argue about what is currently axiomatic and how certain certainties are foundational, but this is only an affirmation of a kind of thinking that needs the external world to conform to our senses and instruments to make meaning. As humans we are always making meaning and mistakes—those things go hand in hand. Like biological life, knowledge is iterative and it evolves. As our knowledge evolves, so too should our way of describing that evolution. The philosophical notion of ontology can be therefore connection to the information science concept of ontology 8 if we are able to represent human knowledge's interrelationships that describe what we know, temporally, to be true for all domains of discourse in as many ways as can be accurately and approximately completed. Such an ontology would reflect in an abstract and concrete way what it is to be human. What would tell us who we are. Science is a bit of a dialectic—there is movement in going from an unknown to a known that has a telos that can and should be represented in an ontology. Therefore, to say that we know something is to make an epistemological statement, which begs an epistemological question: how do you know that? When our understanding of science is as Aschwanden says: Science is not a magic wand that turns everything it touches to truth. Instead, \"science operates as a procedure of uncertainty reduction [...] [t]he goal is to get less wrong over time.\" This concept is fundamental—whatever we know now is only our best approximation of the truth. We can never presume to know everything. 9 That this line of thinking is novel for people in STEM fields is as disconcerting as the lack of statistical knowledge in the humanities. I see this as perhaps the most pervasive mistake in higher education. On the one hand, we are told not to be a generalist; to specialize, to find a niche and exploit it. On the other hand, we miss out on perspectives, ways of thinking, and challenges that provoke deeper thinking than would normally be the case. In working toward teaching myself computer science and mathematics, I am continually pushing myself to broaden my understanding of the world. How are STEM only researchers doing the same? Without a complete education that covers all domains at least in part we lack the tools to truly understand the 21st century. We need interdisciplinary teams of interdisciplinarians; not callow one-faceted domain caricatures that cannot think in multiple loci. I believe this will enable humanity to probe deeper into complexity and how ecologically interdependent we all are on Earth. Aschwanden, C. (2015, August 19). Science Isn't Broken: It's just a hell of a lot harder than we give it credit for. Retrieved August 25, 2015, from https://fivethirtyeight.com/features/science-isnt-broken/ ↩ Tennis, J. T. (2002). \"Subject Ontogeny: Subject Access through Time and the Dimensionality of Classification.\" In Challenges in Knowledge Representation and Organization for the 21st Century: Integration of Knowledge across Boundaries: Proceedings of the Seventh International ISKO Confrence . (Granada, Spain, July 10-13, 2002). Advances in Knowledge Organization, vol. 8. Wurzburg: Ergon: 54-59. ↩ I inherited this opinion from Dr. John McCumber through his various philosophy courses and seminars. See http://www.germanic.ucla.edu/people/faculty/mccumber/ for more info. ↩ https://en.wikiquote.org/wiki/Heraclitus ↩ What I mean is that, when reading the passage, we are in the river; the thing we are trying to classify—to understand. The fact that we have no bird's eye view is an important caveat to working toward understanding the world in a realistic way. We call it one thing, but it is something else outside of the way humans communicate it to each other through language. ↩ Tennis (2002) ↩ McCumber, J. (1989). Poetic interaction: Language, freedom, reason. Chicaco: University of Chicago Press ↩ https://en.wikipedia.org/wiki/Ontology_(information_science) ↩ Aschwanden (2015) ↩","title":"\"P-Hacking and Ontology\"","tags":"Philosophy"},{"url":"http://robertmitchellv.com/blog/bar-chart-annotations-with-pandas-and-mpl.html","text":"When I first started using Pandas, I loved how much easier it was to stick a plot method on a DataFrame or Series to get a better sense of what was going on. However, I was not very impressed with what the plots looked like. Any time I wanted to do something slightly different from the \"Plotting\" documentation on the pydata site, I found myself arm deep in MPL code that did not make any damn sense to me. This was a problem for me, as I ended up spending way too much time trying to make small edits and not enough time working on the code I was trying to visualize. One thing in particular bugged me. I could find no easy to understand tutorial on annotating a bar chart on StackOverflow or any other site. MPL had some documentation, but it was too confusing for me at the time. I spent a lot of time trying to figure out how to put some text right above my bars. Since I would have loved to see a snippet of code to help me in my journey, I thought I would throw it together in a brief post so others could use my workaround. I warn you, it is not the most elegent solution, I am sure, but it worked for me when I needed to demonstrate the insight I had gained from a Healthcare Access and Utilization Survey (made up mostly of CHIS questions) to people in my department, my director, and her bosses. Since I cannot share any of that data, I will use the War of the Five Kings Dataset that Chris Albon made. I love this data set because I am in the middle of book five of Game of Thrones, which provides a good amount of domain familiarity to enable jumping in easier. Setup + Import Data import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline # set jupyter's max row display pd . set_option ( 'display.max_row' , 1000 ) # set jupyter's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) # Load the dataset data = pd . read_csv ( '5kings_battles_v1.csv' ) First visualization with annotations ax = data [ 'region' ] . value_counts () . plot ( kind = 'barh' , figsize = ( 10 , 7 ), color = \"coral\" , fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"Where were the battles fought?\" , fontsize = 18 ) ax . set_xlabel ( \"Number of Battles\" , fontsize = 18 ); ax . set_xticks ([ 0 , 5 , 10 , 15 , 20 ]) # create a list to collect the plt.patches data totals = [] # find the values and append to list for i in ax . patches : totals . append ( i . get_width ()) # set individual bar lables using above list total = sum ( totals ) # set individual bar lables using above list for i in ax . patches : # get_width pulls left or right; get_y pushes up or down ax . text ( i . get_width () +. 3 , i . get_y () +. 38 , \\ str ( round (( i . get_width () / total ) * 100 , 2 )) + '%' , fontsize = 15 , color = 'dimgrey' ) # invert for largest on top ax . invert_yaxis () The image above is the output from the Jupyter notebook. I think it is super clear and gives a lot of information about where the battles were fought. However, I am very parital to horizontal bar charts, as I really think they are easier to read, however, I understand that a lot of people would rather see this chart implemented in a regular bar chart. So, here is the code to do that; you will notice that a few things have changed in order to create the annotation. ax = data [ 'region' ] . value_counts () . plot ( kind = 'bar' , figsize = ( 10 , 7 ), color = \"coral\" , fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"Where were the battles fought?\" , fontsize = 18 ) ax . set_ylabel ( \"Number of Battles\" , fontsize = 18 ); ax . set_yticks ([ 0 , 5 , 10 , 15 , 20 ]) # create a list to collect the plt.patches data totals = [] # find the values and append to list for i in ax . patches : totals . append ( i . get_height ()) # set individual bar lables using above list total = sum ( totals ) # set individual bar lables using above list for i in ax . patches : # get_x pulls left or right; get_height pushes up or down ax . text ( i . get_x () -. 03 , i . get_height () +. 5 , \\ str ( round (( i . get_height () / total ) * 100 , 2 )) + '%' , fontsize = 15 , color = 'dimgrey' ) I play around with the mpl.text() numbers for almost each chart. They are never exactly where they need to be, which often means moving thigs around a hair here and .03 there. You can add or subtract, which means you can also do this: ax = data [ 'attacker_outcome' ] . value_counts () . plot ( kind = 'bar' , figsize = ( 10 , 7 ), color = \"indigo\" , fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"Do attackers usually win or loose?\" , fontsize = 18 ) ax . set_ylabel ( \"Number of Battles\" , fontsize = 18 ); ax . set_yticks ([ 0 , 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 ]) # create a list to collect the plt.patches data totals = [] # find the values and append to list for i in ax . patches : totals . append ( i . get_height ()) # set individual bar lables using above list total = sum ( totals ) # set individual bar lables using above list for i in ax . patches : # get_x pulls left or right; get_height pushes up or down ax . text ( i . get_x () +. 12 , i . get_height () - 3 , \\ str ( round (( i . get_height () / total ) * 100 , 2 )) + '%' , fontsize = 22 , color = 'white' ) If you are like me, often you like to isolate a categorical value in one column and see what the rest of the dataframe looks like in light of that. It is a simply way of drilling down, but a percentage really would not be as appropriate as a count. Here is an example of using a count rather than a percentage: losses = data [ data [ 'attacker_outcome' ] . str . contains ( \"loss\" , na = False )] ax = losses [ 'attacker_king' ] . value_counts () . plot ( kind = 'barh' , figsize = ( 10 , 7 ), color = \"slateblue\" , fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"Who were the attackers who lost?\" , fontsize = 18 ) ax . set_xlabel ( \"Number of Battles\" , fontsize = 18 ); ax . set_xticks ([ 0 , 5 ]) # set individual bar lables using above list for i in ax . patches : # get_width pulls left or right; get_y pushes up or down ax . text ( i . get_width () +. 1 , i . get_y () +. 31 , \\ str ( round (( i . get_width ()), 2 )), fontsize = 15 , color = 'dimgrey' ) # invert for largest on top ax . invert_yaxis () You can also just project a couple columns from those that lost to compare a couple of values; I think bar charts are great for this purpose. I am not sure what the best way would be do accomplish this, but here is my implementation: ax = losses [[ 'attacker_size' , 'defender_size' ]] . plot ( kind = 'bar' , figsize = ( 15 , 7 ), color = [ 'dodgerblue' , 'slategray' ], fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"For Attacker Losses, What was the Difference in Size?\" , fontsize = 18 ) ax . set_ylabel ( \"Number of Troops\" , fontsize = 18 ); ax . set_yticks ([ 0 , 20000 , 40000 , 60000 , 80000 , 100000 , 120000 , 140000 ]) ax . set_xticklabels ([ \"Robb v Joff/Tommen\" , \"Joff/Tommen v Robb\" , \"Stannis v Joff/Tommen\" , \"Robb v Joff/Tommen\" , \"Stannis v Mance\" ], rotation = 0 , fontsize = 11 ) # set individual bar lables using above list for i in ax . patches : # get_x pulls left or right; get_height pushes up or down ax . text ( i . get_x () +. 04 , i . get_height () + 12000 , \\ str ( round (( i . get_height ()), 2 )), fontsize = 11 , color = 'dimgrey' , rotation = 45 ) There is a handy 'rotation' option for the MPL plots that you can use that I feel works well when using a regular bar chart. I really dislike tilting my head to one side to try and read what it says! Also, it is easy to rename the columns! I did not realize how simple it was, which makes me feel silly. Here is the chart done horizontally, which I prefer: ax = losses [[ 'attacker_size' , 'defender_size' ]] . plot ( kind = 'barh' , figsize = ( 10 , 7 ), color = [ 'dodgerblue' , 'slategray' ], fontsize = 13 ); ax . set_alpha ( 0.8 ) ax . set_title ( \"For Attacker Losses, What was the Difference in Size?\" , fontsize = 18 ) ax . set_xlabel ( \"Number of Troops\" , fontsize = 18 ) ax . set_ylabel ( \"First Name is Attacker\" , fontsize = 18 ) ax . set_xticks ([ 0 , 20000 , 40000 , 60000 , 80000 , 100000 , 120000 , 140000 ]) ax . set_yticklabels ([ \"Robb v Joff/Tommen\" , \"Joff/Tommen v Robb\" , \"Stannis v Joff/Tommen\" , \"Robb v Joff/Tommen\" , \"Stannis v Mance\" ]) # set individual bar lables using above list for i in ax . patches : # get_width pulls left or right; get_y pushes up or down ax . text ( i . get_width () + 700 , i . get_y () +. 18 , \\ str ( round (( i . get_width ()), 2 )), fontsize = 11 , color = 'dimgrey' ) # invert for largest on top ax . invert_yaxis () I hope this is helpful for anyone out there trying to create little annotations for their visualizations. I feel like this is just a little bit of extra work but it keeps me from having to write JavaScript, which is worth a little copy paste action. When I have time, I would like to create a class with methods so I do not have to keep doing a copy/paste job in my Jupyter notebook. Let me know if there is an easier way to do this, I would be grateful! Here is a link to the notebook on my GitHub if you are interested in playing with it a bit more. I stopped when I was trying to figure out how to turn the dates into a Pandas 'period_range'.","title":"\"Bar Chart Annotations with Pandas and MPL\"","tags":"Python"},{"url":"http://robertmitchellv.com/blog/my-pandas-snippits-always-evolving.html","text":"As I learn more this page will be updated (and hopefully improved!) The goal of this post is to keep me from googling pandas questions that I've forgotten. I don't know how many times I've looked at the results and seen five or more StackOverflow links that have clearly already been clicked on; I feel like Sisyphus when this happens! So, here is what I'm currently committing to memory: ### Make matplotlib.pyplot look better with no effort: import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline ### Delete column del df [ 'colName' ] ### Rename columns df . columns = [ 'col1' , 'col2' , 'col3' ] # this does not reindex columns ### Combine columns df [ 'newCol' ] = df [ 'col1' ] . map ( str ) + data [ 'col2' ] + data [ 'col3' ] . astype ( 'str' ) ### Copy column df [ 'newCol' ] = df [ 'oldCol' ] # where newCol is the copy ### Reindex columns cols = [ 'col1' , 'col2' , 'col3' , 'col4' ] # list of how you'd like it df = df . reindex ( columns = cols ) ### Find out how many NaN values you have in a column df [ 'colName' ] . isnull () . sum () ### Show unique values df [ df [ 'colName' ] . unique ()] ### Create a frequency column from another column df [ 'freq' ] = df . groupby ( 'colName' )[ 'colName' ] . transform ( 'count' ) ### Delete row df = df . drop ( 2 ) # where two is the df's index df = df . drop ( 'rowName' ) # if you reindexed ### Remove characters before a specific character df [ 'colName' ] = df [ 'colName' ] . apply ( lambda x : x . split ( '-' )[ - 1 ]) # char = - ### Remove characters after a specific character df [ 'colName' ] = df [ 'colName' ] . apply ( lambda x : x . split ( '-' )[ 0 ]) # char = - ### Remove characters, e.g., commas from data df [ 'colName' ] = df [ 'colName' ] . str . replace ( ',' , '' ) ### Convert datatypes, e.g., object to float df [[ 'col4' , 'col5' , 'col10' ]] = df [[ 'col4' , 'col5' , col10 ]] . astype ( float ) ### Convert string date to datetime64 df [ 'strDate' ] = pd . to_datetime ( df [ 'strDate' ]) ### Filter datetime64 column values import datetime df [ df [ 'colName' ] >= datetime . date ( 2015 , 1 , 1 )] ### Convert NaN values to zeros (or anything else) df = df . fillna ( 0 ) # remember that this returns a new object! ### Replace string values with numeric representations dictionary = { 'value1' : 1 , 'value2' : 2 , 'Value3' : 3 } df = df . replace ({ 'colName' : dictionary }) ### Replace multiple cells of a column only with a different string df . loc [ df [ 'colName' ] . str . contains ( 'word' ), df [ 'colName' ]] = \"Different Word\" # or df . loc [ df [ 'colA' ] . str . contains ( 'word' ), [ 'colB' ]] = 5 # to change a cell in a different column ### Project data based on a value range from a column df [ df . colWithNumbers <= 360 ] # shows me values less than or equal to 360 df [ df [ 'colWithStrings' ] . str . contains ( \"word\" )] # shows me values with 'word' in them ### Project data based on two values (use and or pipe symbol to denote relationship) df [( df [ 'colWithString' ] . str . contains ( \"word\" )) & ( df . colWithNumber <= 5 )] # and df [( df [ 'colWithString' ] . str . contains ( \"firstWord\" )) | ( df [ 'colWithString' ] . str . contains ( \"secondWord\" ))] # or ### Groupby as variable groupedby = df . groupby ( df . colName ) # or: groupedby = df . groupby ( df . colName ) . add_suffix ( '/Mean' ) # add column suffixes ### Use groupedby variable and find the mean for your values groupedbyMean = groupedby . mean ()","title":"\"My Pandas Snippits—always evolving\"","tags":"Python"},{"url":"http://robertmitchellv.com/blog/first-kaggle-submission-random-forest-classifier.html","text":"I have seen kaggle mentioned on twitter a lot; mostly by the data scientists and researchers I look up to, but there's never been much confidence that the site was for me in any way—mostly because I was a long way from my dream data science job with yet so much to learn. Notwithstanding, I cannot help but try and hack my way to my destination! I think it's a part of my learning process: thrust myself in the midst of something I don't understand, get stuck, try to get unstuck, finish with some understanding of what I was doing. So, when I saw this post by Chris Clark , I thought that it was about time I try and hack my way from recently learning Python to machine learning with SciKit-Learn—why not!?—I thought. It reminded me of when I decided to sign up with an account at GitHub; I was initially intimidated because it was new to me. Now, I use git in the command line, host my website there, and use it for almost everything (still learning new things about git everyday as well). Chris's post was excellent but there was one problem: the code was aimed at Python 2.7 users and I had just spent the previous semester learning Python 3 (which means I don't really know 2.7; and avoid it all the time \"where are the parens for this print statement??\"). As a personal challenge, I decided to use the code and update it to Python 3, which was both fun and challenging (I'm measuring 'update' to mean, 'running in my Python 3.4 interpreter without error messages'). This may be an easy task but there were a few snags for me. In the spirit of trying to document the things I learn, I've decided to chronical my results here—if there are any errors or issues with this code, please let me know so I can try to correct, learn, and grow! I also found Chris's updated code on GitHub, which uses Pandas and I've been trying to get started with Pandas as well so; win, win. As an aside, I use Anaconda and Vim for the enviornment and editing, respectively. My code can be found on GitHub . The Submission was a part of the Predicting a Biological Response competition, and the training, test, and benchmark data sets are provided. Since the competition wants us to predict binary values, Chris notes that this data set is a good introduction to ensemble classifiers, because the prediction is a binary value (0 or 1). It was also great to take a closer look at both the Pandas and SciKit-Learn's documentation to troubleshoot. I tried to use the comments to explain as much as possible so future me will not be baffled, which I can say is helpful since I'm looking at this one month out and it makes total sense (at least to me). ### Kaggle Submission Code \"\"\" //kaggle submission //Biological Response --> random forest classifier Author: Robertmitchellv Date: Dec 16, 2104 Revised: Dec 22, 2014 \"\"\" import pandas as pd from sklearn.ensemble import RandomForestClassifier def main (): # create the training + test sets try : data = pd . read_csv ( 'Data/train.csv' ) except IOError : print ( \"io ERROR-->Could not locate file.\" ) target = data . Activity . values train = data . drop ( 'Activity' , axis = 1 ) . values test = pd . read_csv ( 'Data/test.csv' ) . values # create and train the random forest and call it 'rf' # --> n_estimators = the number of trees in this forest, viz. # 100 trees of forest # --> n_jobs set to -1 will use the number of cores present on your system. rf = RandomForestClassifier ( n_estimators = 100 , n_jobs = - 1 ) # fit(X, y[, sample_weight]) = build a forest of tress from the # training set (X, y) rf . fit ( train , target ) # predict_proba(X) predict class probabilities for X as list predicted_probs = [ x [ 1 ] for x in rf . predict_proba ( test )] # prep data for use in pd.Series molID , predictProbs = prepData ( predicted_probs ) # use a dictionary with keys as col headers and values as lists pulled from # previous prep function df = { 'MoleculeID' : molID , 'PredictedProbability' : predictProbs } # pandas DataFrame = a tabular datastructure like a SQL table predicted_probs = pd . DataFrame ( df ) # write predicted_probs to file with pandas method .to_csv()--add header # for submission try : predicted_probs . to_csv ( 'Data/submission.csv' , index = False ) print ( \"File successfully written; check 'Data' folder\" ) except IOError : print ( \"io ERROR-->Could not write data to file.\" ) # preparing data for conversion to pd.DataFrame def prepData ( alist ): # prepare list to be converted to pandas Series colOne = [] colTwo = [] idx = 1 # for loop to set MoleculeID to match the benchmark; # place values into list for easier wrangling as pd.Series for i in alist : colOne . append ( idx ) colTwo . append ( i ) idx += 1 return colOne , colTwo # call the main function main () After performing this--Chris suggested to submit to kaggle; being an extra careful person by nature, I just had to perform the evaluation and cross validation first (I don't know if any of you feel the same way). Unfortunately, I don't really understand how the code works--this is one of the problems when hacking through tutorials. ### Evaluation/Logloss \"\"\" //kaggle submission //Biological Response --> evaluation function (from Grunthus' post) \"\"\" import scipy as sp def logloss ( act , pred ): \"\"\" Vectorised computation of logloss \"\"\" #cap in official Kaggle implementation, #per forums/t/1576/r-code-for-logloss epsilon = 1e-15 pred = sp . maximum ( epsilon , pred ) pred = sp . minimum ( 1 - epsilon , pred ) #compute logloss function (vectorised) ll = sum ( act * sp . log ( pred ) + sp . subtract ( 1 , act ) * sp . log ( sp . subtract ( 1 , pred ))) ll = ll * - 1.0 / len ( act ) return ll The cross validation was trickier to understand, which I think is mostly due to my not really understanding what ensemble classifiers do, how the random forest classifier works, and more specifically; what training, test, and target data do within machine learning. This gave chase through the SciKit-Learn documentation and other resources online to get a better understanding of what the code was doing—there's a lot to learn! The interesting aspect is how the SciKit-Learn reserves some actual data that it can test against the classifier's predicted values. I tried to show in the comments how I was understanding what the code did at the time. ### Cross Validation \"\"\" //kaggle submission //Biological Response --> cross validation \"\"\" from sklearn.ensemble import RandomForestClassifier from sklearn.cross_validation import KFold import numpy as np import pandas as pd import logloss def main (): #read data from csv; use nparray to create the training + target sets try : train = pd . read_csv ( 'Data/train.csv' ) except IOError : print ( \"io ERROR-->Could not locate file.\" ) target = np . array ([ x [ 0 ] for x in train ]) train = np . array ([ x [ 1 :] for x in train ]) # in this case we'll use a random forest, but this could be any classifier model = RandomForestClassifier ( n_estimators = 100 , n_jobs = - 1 ) # simple K-Fold cross validation. 10 folds. cv = KFold ( n = len ( train ), n_folds = 10 , indices = False ) #iterate through the training and test cross validation segments and #run the classifier on each one, aggregating the results into a list results = [] for traincv , testcv in cv : prob = model . fit ( train [ traincv ], target [ traincv ]) . predict_proba ( train [ testcv ]) results . append ( logloss . llfun ( target [ testcv ], [ x [ 1 ] for x in prob ])) #print out the mean of the cross-validated results print ( 'Results: ' , str ( np . array ( results ) . mean ())) # call main function main () After I was able to execute the submission, logloss, and cross validation code without any errors, I submitted my code to kaggle. It was an exciting moment waiting to see what kind of score I would have recieved had I actually participated in the competition. I would have placed at 325 (well, I would have tied with another user for 325th); check out my results below. Well, that wraps up my first submission to kaggle. I really hope this is the first of many. Right now I'm working through the Think Stats + Think Bayes books to refresh my stats knowledge. I'm trying to find time to work on the Titanic tutorial through kaggle as well as perhaps throw a hat in the ring for Booz Hamilton's Data Science Bowl. There's so much to learn and I can't wait for these concepts to become more natural and familiar.","title":"\"First Kaggle Submission—Random Forest Classifier\"","tags":"Python"},{"url":"http://robertmitchellv.com/blog/the-python-journey-one-semester-with-python-34.html","text":"This was quite a journey for me. I started the same way everyone else has; with my very first \"Hello World\" program written in Python 3: # Hello World! program. def main (): #get the user's name name = input ( 'What is your name? ' ) print ( 'Hello World! I am' , name ) # Call the main function main () This was the first assignment for my Programming in Python course. I was not content to have it only print \"Hello World\"—no, I need to personalize it in some small way. The following was really interface (if I'm speaking pythonically); to a wider interest in programming qua programming: name = input ( 'What is your name? ' ) Nevertheless, I was not content with this. I allowed myself to be sucked into a forceful vortex that had me thinking I'd be using Jupytr notebooks, matplotlib, etc., to show off how much I know about Python from Twitter. Notwithstanding, the above is what was submitted because I didn't know how to do any of the fancy stuff I read about. I didn't know how to use Pandas. I didn't know how to use Blaze. I didn't even know how to use 'conda update conda' in my terminal (oh; it's a package manager—not just an easy way to install Python 3.4 on my computer at work without Admin privlages!). The reality is that I still have a lot to learn—I'm still in the shallow end. Nothing prepared me for the absolute angst associated with trying to implement a (beginner's attempt at) the Object Oriented Programming (OOP) paradigm as a final extra-credit assignment! I didn't even know I had been writing, although very functional; or, very function reliant, procedural code. Somewhere between nesting lists inside of dictionaries, iterating over them, and implementing 'try, except' statements, I thought I was really going places with my code. OOP razed that sandcastle quite briskly. Like a kind of soverign and violent natural phenomena. From my first program to my 10th program, this is how far I have come. This is my attempt at OOP, classes, 'init' methods, ~~inheritance~~ composition, and more. It's likely pretty flawed and could be made less redundant, but I didn't copy StackOverflow and tried to figure it out on my own; so, I'm damn proud of it! There were some programs specs that I needed to show an understanding of; quickly, the program specs: Each question will have four possible answers Each player will take turns with the questions There will be a total of 10 questions, each player getting a chance to answer five of them. If the player selects the correct answer, they earn a point. Tell the player whether they got it right or wrong. Must create a 'Question' class to hold data with the following attributes: A trivia question Possible answer 1 Possible answer 2 Possible answer 3 Possible answer 4 The number of the correct answer, e.g., 1, 2, 3, or 4 Question class must have an ' init ' method, accessors, mutators, and a ' str ' method. Use value-returning functions; one named createQuestionsAnswers() that creates the list to display questions and keeps tracks of user input to let players know if they won, lost, or tied. Here are my solutions: # -*- coding: utf-8 -*- \"\"\" A10--Trivia Game! --> two player trivia game --> OOP approach to building the game with classes and objects \"\"\" import csv import random # the Question class acts as a placeholder for the parts of the question # needed to construct questions and check answers class Question : # __init__ uses the Data class method getData through composition def __init__ ( self , question , a1 , a2 , a3 , a4 , answer , ansNum ): self . getData = Data ( 'csv' ) self . question = question self . a1 = a1 self . a2 = a2 self . a3 = a3 self . a4 = a4 self . answer = answer self . ansNum = ansNum # the method performs better as a class method since it instantiates the # Question class with sample questions for the game @classmethod def getQuestion ( cls , triviaDict ): # using random to get 10 random numbers between a specific range for # trivia questions randomGenerator = random . sample ( range ( 1 , 817 ), 1 ) # for an individual random number in the sample range # --> iterate and use number as index for the trivia questions for i in randomGenerator : question = triviaDict [ i ][ 0 ] a1 = triviaDict [ i ][ 1 ] a2 = triviaDict [ i ][ 2 ] a3 = triviaDict [ i ][ 3 ] a4 = triviaDict [ i ][ 4 ] answer = triviaDict [ i ][ 5 ] ansNum = triviaDict [ i ][ 6 ] # this creates an instance to return (from question class) aQuestion = Question ( question , a1 , a2 , a3 , a4 , answer , ansNum ) return aQuestion # this is a part of using composition rather than inheritance to get the # attributes from the getData method def __getattr__ ( self , attr ): return getattr ( self . getData , attr ) # this method sets up the question (also checks answer) @classmethod def setupAsk ( cls , q ): print ( ' \\n ' , q . question , ' \\n\\t 1: ' , q . a1 , ' \\ \\n\\t 2: ' , q . a2 , ' \\n\\t 3: ' , q . a3 , ' \\ \\n\\t 4: ' , q . a4 , ' \\n ' ) # make sure the user's input works while True : try : choice = int ( input ( \" \\n What's your answer? \\n --> \" )) except ValueError : print ( ' \\n Sorry, the answer only accepts numbers; please \\ enter a number 1-4' ) choice = int ( input ( \" \\n What's your answer? \\n --> \" )) finally : if choice in range ( 1 , 5 ): break # if the question is correct, return true; if not, return false if choice == q . ansNum : print ( ' \\n Correct! \\n ' , q . answer ) return True elif choice != q . ansNum : print ( ' \\n Incorrect! \\n ' , q . answer ) return False # the Data class handles openning the file and preparing it to be used by # the Question and Game class class Data : def __init__ ( self , filetype ): self . filetype = filetype # opens the CSV to read and prepare it to be used in computaiton later @classmethod def getData ( cls ): # make sure there isn't an IO error try : # open the csv file + use an index accumulator for dictionary with open ( 'trivia.csv' ) as csvFile : readCSV = csv . reader ( csvFile , delimiter = ',' ) index = 0 # questions, answer choices, and answers dictionary rowDict = {} questionData = {} # reading the trivia questions for row in readCSV : rowDict [ index ] = row question = rowDict [ index ][ 0 ] a1 = rowDict [ index ][ 1 ] a2 = rowDict [ index ][ 2 ] a3 = rowDict [ index ][ 3 ] a4 = rowDict [ index ][ 4 ] answer = rowDict [ index ][ 5 ] # figure out which answer is correct and assign a variable if answer == a1 : ansNum = 1 elif answer == a2 : ansNum = 2 elif answer == a3 : ansNum = 3 elif answer == a4 : ansNum = 4 else : print ( \"Error! No correct answer\" ) # place questions into new dictionary in the right order questionData [ index ] = [ question , a1 , a2 , a3 , a4 , \\ answer , ansNum ] index += 1 return questionData except IOError : print ( \"The file could not be found.\" ) # the Game class is where the bulk of the game's structure is found class Game : def __init__ ( self , playerID , gamePoints ): self . playerID = playerID self . gamePoints = gamePoints # method to create instances for questions and find out if a quesiton # was answered correctly or not def round ( self , qClass , data ): gamePoints = 0 # reset to 0 for new round # instances q1 = qClass . getQuestion ( data ) q2 = qClass . getQuestion ( data ) q3 = qClass . getQuestion ( data ) q4 = qClass . getQuestion ( data ) q5 = qClass . getQuestion ( data ) # return value is true or false; this computes points if qClass . setupAsk ( q1 ) == True : gamePoints += 1 if qClass . setupAsk ( q2 ) == True : gamePoints += 1 if qClass . setupAsk ( q3 ) == True : gamePoints += 1 if qClass . setupAsk ( q4 ) == True : gamePoints += 1 if qClass . setupAsk ( q5 ) == True : gamePoints += 1 # let the user know what happenned this round print ( 'you won {} points this game!' . format ( gamePoints )) return gamePoints def main (): # local variables flag = False gameNum = 1 # instance of Data class data = Data ( 'csv' ) questionsData = data . getData () # instance of Question class with filler data questions = Question ( 'question' , 'a1' , 'a2' , 'a3' , 'a4' , \\ 'answer' , 'ansNum' ) # create both players playerOne = Game ( str ( input ( 'PLAYER ONE// \\n Enter your name: ' )), 0 ,) playerTwo = Game ( str ( input ( 'PLAYER TWO// \\n Enter your name: ' )), 0 ,) # while loop to keep the game going if the user chooses while flag != True : # let the user know which round they're playing print ( ' \\n ROUND ' , gameNum , '// \\n Player One' ) # first player instance; asks five questions p1round = playerOne . round ( questions , questionsData ) print ( \"\"\" +++++++++++++++++++++++++++++++++++++++++++++++++ + SWITCH PLAYERS! + +++++++++++++++++++++++++++++++++++++++++++++++++ \"\"\" ) # let the user know to switch players print ( ' \\n ROUND ' , gameNum , '// \\n Player Two' ) # second player instance; asks the five quesitons p2round = playerTwo . round ( questions , questionsData ) # let the user know which round they are on with accumulator gameNum += 1 # figure out who won and use user's inputed name and their points # in print statement if p1round < p2round : print ( 'Thank you for playing! {} is the winner with {} total \\ game points!' . format ( playerTwo . playerID , p2round )) elif p2round < p1round : print ( 'Thank you for playing! {} is the winner with {} total \\ game points!' . format ( playerOne . playerID , p1round )) elif p2round == p1round : print ( 'There was a tie! Both {} and {} both earned {} total \\ game points; but you are both still \\ winners!' . format ( playerOne . playerID , playerTwo . playerID , \\ p1round )) # find out if user wants to continue + validate user response while True : try : choice = str ( input ( \" \\n Keep playing? \\n --> \" )) . upper () except ValueError : print ( \"Sorry, enter either a 'Y' for 'Yes', or 'N' for 'No'.\" ) finally : if choice == 'N' : flag = True break if choice == 'Y' : break else : print ( \"please enter either a 'Y' for 'Yes', or 'N' \\ for 'No'.\" ) # say bye to players and quit the program print ( ' \\n Thank you for playing! See you next time! \\n ' ) main () I couldn't help but think about Plato when I was trying to understand how Objects work in Python. There's something really similar about how a 'class' has a kind of ontos —that it isn't just a blueprint—it exists, and it did exist before the 'init' method gave it attributes; that prescriptive human speculation we come up with when describing the form of something abstract like 'the Beautiful' (although it's been a while since my Phil 100A course at UCLA—I hope I'm not misrepresenting the Phaedo ). The course is over but I have a few titles I purchased from Packt to dig a little deeper. Suggestions are always welcome; the journey's telos is to learn; and learn I intend to do!","title":"\"The Python Journey—One Semester with Python 3.4\"","tags":"Python"},{"url":"http://robertmitchellv.com/blog/foucaults-challenge-to-modernist-classification.html","text":"In Foucault's Les Mots et les choses (The Order of Things), he notes a passage in Borges that, for him, demonstrates the limitations of taxonomic assertions in the face of exotic systems of thought—via Borges, he quotes a ‘certain Chinese encyclopedia' in which it is written: animals are divided into: (a) belonging to the Emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies 1 What shocks Foucault about this passage is what connects these categories—i.e., the structure that links these strange juxtaposed oddities: the alphabetical series 2 . Foucault questions on what bedrock would kinship between ‘(i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush' -animals meet, other than the non-space of the utterance itself; i.e., the non-space of language 3 . Language can only display this kinship in an ‘unthinkable space' (abstract locus)—Borges wants to remove the ‘operating table' that enables thought to order, divide, and classify external entities 4 . Effectively, this removes the ground upon which, \"since the beginning of time, language has intersected space\" 5 . Borges' works often lies in the abstract space of the ‘heterotopia' 6 , which \"desiccates speech, stops words in their tracks, contests the very possibility of grammar at its source; […] [to] dissolve our myths and sterilize the lyricism of our sentences\" 7 . This highlights the challenge of classifying in the post-modern library, for now there is an unforeseen danger—not incongruous disorder but the linking together of things that are inappropriate [&#94;8]. A perfect classification scheme that represents a universe of knowledge is the pipe-dream of the modernist; our observations are not independent of the external world, which undermines our ability to classify. We are enmeshed in our world; contextualized in the milieu that is the object of our analysis. We have no bird's eye view—our objectivity has no locus from which to observe. Nevertheless, I feel that there is a space to enhance knowledge organization. First, I believe it is important to shed the illusion of a temporal permanence of facts, which is not solid and more fluid. Meaning, our reconstructions of the external world mirrored by our knowledge organization schemes change through time and are in flux. Second, I believe it is important to increase transparency and acknowledge bias which can exist through ethnocentrism, race, religion, gender, sex, power, language, geography, et cetera. Effectively this is similar to Jung's notion of the shadow, that as humans we feel that it is silly to believe we cannot accurately describe the external world—we all agree on things, we test them, and we derive data with which to harness confidence in talking objectively about the external world. We do this daily. It is this confidence that leaves us blind to the shadow of our foundation; like Venice, we are sinking. In order to bring ourselves back to a form of equilibrium we must admit that our shadow exists, namely, that we bring as much to our observations than we leave and, in light of this; we should attempt to root out future bias through honesty and self-understanding. In this way, we stand the chance of building more honest reflections of knowledge. And, again, in this way, we stand the chance of achieving some kind of universe of human knowledge, or at least a good representation of what we feel that we know. Foucault, M. 2002. The order of things an archaeology of the human sciences. London and New York: Routledge Classics (Original work published 1966). ↩ Ibid ↩ Ibid ↩ Ibid ↩ Ibid ↩ \"Different from utopias, which also have no locality, heterotopias are disturbing because they undermine language, make impossible the naming of ‘this' or ‘that', because they shatter both the syntax that humans construct sentences with, as well as the syntax that holds words and things together\" ↩ Foucault, 2002 ↩","title":"\"Foucault's Challenge to Modernist Classification\"","tags":"Philosophy"}]}